{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a75e5759",
   "metadata": {},
   "source": [
    "# Study 1 Replication Analysis\n",
    "\n",
    "This notebook replicates all results from Study 1 of the article, systematically going through each analysis and statistic reported.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f138bd5f",
   "metadata": {},
   "source": [
    "## Imports and Configuration\n",
    "\n",
    "Import necessary libraries and set up helper functions for the analyses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dd9374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import ttest_rel, ttest_ind, pearsonr\n",
    "import warnings\n",
    "import pingouin as pg\n",
    "\n",
    "\n",
    "# Ignore warnings - for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Helper function: Spearman-Brown correction for split-half reliability\n",
    "def spearman_brown(r):\n",
    "    \"\"\"Apply Spearman-Brown correction to split-half reliability.\"\"\"\n",
    "    return (2 * r) / (1 + r)\n",
    "\n",
    "# Helper function: Fisher z transformation for comparing correlations\n",
    "def fisher_z(r):\n",
    "    \"\"\"Fisher z transformation.\"\"\"\n",
    "    return 0.5 * np.log((1 + r) / (1 - r))\n",
    "\n",
    "def compare_correlations(r1, r2, n1, n2):\n",
    "    \"\"\"\n",
    "    Compare two independent correlations using Fisher z test.\n",
    "    \n",
    "    Parameters:\n",
    "    r1, r2: correlation coefficients\n",
    "    n1, n2: sample sizes\n",
    "    \n",
    "    Returns:\n",
    "    z: z-statistic\n",
    "    p: p-value (two-tailed)\n",
    "    \"\"\"\n",
    "    z1 = fisher_z(r1)\n",
    "    z2 = fisher_z(r2)\n",
    "    se_diff = np.sqrt(1/(n1-3) + 1/(n2-3))\n",
    "    z = (z1 - z2) / se_diff\n",
    "    p = 2 * (1 - stats.norm.cdf(abs(z)))\n",
    "    return z, p\n",
    "\n",
    "# Helper function: Cohen's d for paired samples\n",
    "def cohens_d_paired(x1, x2):\n",
    "    \"\"\"Calculate Cohen's d for paired samples.\"\"\"\n",
    "    diff = x1 - x2\n",
    "    d = diff.mean() / diff.std(ddof=1)\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b425fe20",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Load the combined participants file and filter to Study 1 only. Handle exclusions and verify sample sizes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9cd8a3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total participants in combined file: 898\n",
      "Study 1 participants (before exclusions): 298\n",
      "\n",
      "Exclusion column value counts:\n",
      "exclude\n",
      "0    259\n",
      "1     39\n",
      "Name: count, dtype: int64\n",
      "Study 1 participants (after exclusions): 259\n",
      "\n",
      "Expected N = 262 (after exclusions)\n",
      "Actual N = 259\n",
      "\n",
      "Group distribution:\n",
      "group\n",
      "0    89\n",
      "1    85\n",
      "2    85\n",
      "Name: count, dtype: int64\n",
      "(0 = Control, 1 = Faking Low/Introversion, 2 = Faking High/Extraversion)\n",
      "\n",
      "Task distribution:\n",
      "task\n",
      "IAT     140\n",
      "qIAT    119\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Dataframe ready for analysis: 259 participants\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "BASE_DIR = Path('..')\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "\n",
    "# Load the combined participants file\n",
    "combined_file = DATA_DIR / \"combined_participants_S1&S2.csv\"\n",
    "df_all = pd.read_csv(combined_file)\n",
    "\n",
    "print(f\"Total participants in combined file: {len(df_all)}\")\n",
    "\n",
    "# Filter to Study 1 only\n",
    "# Study column might be 'S1' or '1', so we'll check both\n",
    "df_s1 = df_all[df_all['study'].astype(str).str.upper().isin(['S1', '1'])].copy()\n",
    "\n",
    "print(f\"Study 1 participants (before exclusions): {len(df_s1)}\")\n",
    "\n",
    "# Handle exclusions\n",
    "# The 'exclude' column indicates if participant should be excluded from entire study\n",
    "if 'exclude' in df_s1.columns:\n",
    "    # Check what values indicate exclusion (likely True/1 for excluded)\n",
    "    print(f\"\\nExclusion column value counts:\")\n",
    "    print(df_s1['exclude'].value_counts())\n",
    "    \n",
    "    # Filter out excluded participants\n",
    "    # Assuming True, 1, or 'True' means excluded\n",
    "    df_s1_included = df_s1[~df_s1['exclude'].astype(str).str.upper().isin(['TRUE', '1', 'YES'])].copy()\n",
    "    print(f\"Study 1 participants (after exclusions): {len(df_s1_included)}\")\n",
    "else:\n",
    "    print(\"Warning: 'exclude' column not found. Using all participants.\")\n",
    "    df_s1_included = df_s1.copy()\n",
    "\n",
    "# Verify sample size matches article (N = 262 after exclusions)\n",
    "print(f\"\\nExpected N = 262 (after exclusions)\")\n",
    "print(f\"Actual N = {len(df_s1_included)}\")\n",
    "\n",
    "# Convert key columns to appropriate types\n",
    "df_s1_included['id'] = df_s1_included['id'].astype(str)\n",
    "df_s1_included['study'] = df_s1_included['study'].astype(str)\n",
    "df_s1_included['task'] = df_s1_included['task'].astype(str)\n",
    "df_s1_included['group'] = pd.to_numeric(df_s1_included['group'], errors='coerce')\n",
    "\n",
    "# Check group distribution\n",
    "print(f\"\\nGroup distribution:\")\n",
    "print(df_s1_included['group'].value_counts().sort_index())\n",
    "print(\"(0 = Control, 1 = Faking Low/Introversion, 2 = Faking High/Extraversion)\")\n",
    "\n",
    "# Check task distribution\n",
    "print(f\"\\nTask distribution:\")\n",
    "print(df_s1_included['task'].value_counts())\n",
    "\n",
    "# Store as main dataframe for analyses\n",
    "df = df_s1_included.copy()\n",
    "print(f\"\\nDataframe ready for analysis: {len(df)} participants\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8283094a",
   "metadata": {},
   "source": [
    "## Load Task Data Files\n",
    "\n",
    "Load all task data files from the tasks/S1 folder. These files contain detailed trial-level data and participant-level statistics (D-scores, split-half reliability data, etc.) that may be needed for analyses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "71170e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading qiat1.xlsx...\n",
      "  Loaded 149 participants\n",
      "  Columns: ['id', 'nTrials', 'maxLatency', 'percFast', 'percError', 'exclude', 'condition', 'n_block_4', 'mean_block_4', 'variance_block_4', 'n_block_6', 'mean_block_6', 'variance_block_6', 'pooled_sd', 'dscore', 'dscore_even', 'dscore_odd', 'dscore_paired_even', 'dscore_paired_odd', 'dscore_random_half1', 'dscore_random_half2']\n",
      "  Loaded 29800 trials\n",
      "Loading qiat2.xlsx...\n",
      "  Loaded 149 participants\n",
      "  Columns: ['id', 'nTrials', 'maxLatency', 'percFast', 'percError', 'exclude', 'condition', 'n_block_4', 'mean_block_4', 'variance_block_4', 'n_block_6', 'mean_block_6', 'variance_block_6', 'pooled_sd', 'dscore', 'dscore_even', 'dscore_odd', 'dscore_paired_even', 'dscore_paired_odd', 'dscore_random_half1', 'dscore_random_half2']\n",
      "  Loaded 29800 trials\n",
      "Loading iat1.xlsx...\n",
      "  Loaded 149 participants\n",
      "  Columns: ['id', 'nTrials', 'maxLatency', 'percFast', 'percError', 'exclude', 'condition', 'n_block_4', 'mean_block_4', 'variance_block_4', 'n_block_6', 'mean_block_6', 'variance_block_6', 'pooled_sd', 'dscore', 'dscore_even', 'dscore_odd', 'dscore_paired_even', 'dscore_paired_odd', 'dscore_random_half1', 'dscore_random_half2']\n",
      "  Loaded 18029 trials\n",
      "Loading iat2.xlsx...\n",
      "  Loaded 149 participants\n",
      "  Columns: ['id', 'nTrials', 'maxLatency', 'percFast', 'percError', 'exclude', 'condition', 'n_block_4', 'mean_block_4', 'variance_block_4', 'n_block_6', 'mean_block_6', 'variance_block_6', 'pooled_sd', 'dscore', 'dscore_even', 'dscore_odd', 'dscore_paired_even', 'dscore_paired_odd', 'dscore_random_half1', 'dscore_random_half2']\n",
      "  Loaded 18029 trials\n",
      "\n",
      "Task files loaded successfully!\n",
      "Available task data:\n",
      "  qIAT Time 1: 149 participants\n",
      "  qIAT Time 2: 149 participants\n",
      "  IAT Time 1: 149 participants\n",
      "  IAT Time 2: 149 participants\n"
     ]
    }
   ],
   "source": [
    "# Define tasks directory\n",
    "TASKS_DIR = DATA_DIR / \"tasks\" / \"S1\"\n",
    "\n",
    "# Task file mapping: (task_type, time) -> file_path\n",
    "task_files = {\n",
    "    ('qIAT', 1): TASKS_DIR / \"qiat1.xlsx\",\n",
    "    ('qIAT', 2): TASKS_DIR / \"qiat2.xlsx\",\n",
    "    ('IAT', 1): TASKS_DIR / \"iat1.xlsx\",\n",
    "    ('IAT', 2): TASKS_DIR / \"iat2.xlsx\",\n",
    "}\n",
    "\n",
    "# Dictionary to store loaded task data\n",
    "# Structure: tasks[task_type][time] = {'participants': df, 'trials': df (if available)}\n",
    "tasks = {\n",
    "    'qIAT': {},\n",
    "    'IAT': {}\n",
    "}\n",
    "\n",
    "# Load each task file\n",
    "for (task_type, time), file_path in task_files.items():\n",
    "    if not file_path.exists():\n",
    "        print(f\"Warning: {file_path} not found, skipping...\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Loading {file_path.name}...\")\n",
    "    \n",
    "    # Load Participants sheet (contains participant-level statistics)\n",
    "    try:\n",
    "        participants_df = pd.read_excel(file_path, sheet_name='Participants')\n",
    "        participants_df['id'] = participants_df['id'].astype(str)\n",
    "        tasks[task_type][time] = {'participants': participants_df}\n",
    "        print(f\"  Loaded {len(participants_df)} participants\")\n",
    "        \n",
    "        # Check what columns are available\n",
    "        print(f\"  Columns: {list(participants_df.columns)}\")\n",
    "        \n",
    "        # Try to load Trials sheet if it exists (contains trial-level data)\n",
    "        try:\n",
    "            trials_df = pd.read_excel(file_path, sheet_name='Trials')\n",
    "            tasks[task_type][time]['trials'] = trials_df\n",
    "            print(f\"  Loaded {len(trials_df)} trials\")\n",
    "        except:\n",
    "            print(f\"  No Trials sheet found (or error loading it)\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  Error loading {file_path}: {e}\")\n",
    "\n",
    "print(f\"\\nTask files loaded successfully!\")\n",
    "print(f\"Available task data:\")\n",
    "for task_type in ['qIAT', 'IAT']:\n",
    "    for time in [1, 2]:\n",
    "        if time in tasks[task_type]:\n",
    "            print(f\"  {task_type} Time {time}: {len(tasks[task_type][time]['participants'])} participants\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87914900",
   "metadata": {},
   "source": [
    "## Sample Size and Demographics\n",
    "\n",
    "Compute total sample size, exclusions, and final sample demographics for Study 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "796e78b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Total S1 N: 298\n",
      "   Total excluded (from exclude column): 39\n",
      "   Should equal: 39 = 39\n",
      "\n",
      "2. Excluded based on implicit task performance: 33\n",
      "   (exclude_t1: 23, exclude_t2: 24)\n",
      "3. Additional exclusions from manipulation groups (failed checks): 7\n",
      "   (failed 1st check: 3, failed 2nd check: 7)\n",
      "   Note: 1 participants excluded for BOTH task performance AND manipulation checks\n",
      "   Calculated total (task + manipulation - overlap): 39\n",
      "   Actual total excluded: 39\n",
      "\n",
      "4. Final N (after all exclusions): 259\n",
      "\n",
      "Gender value counts:\n",
      "gender\n",
      "Female                        138\n",
      "Male                          119\n",
      "other/ I prefer not to say      2\n",
      "Name: count, dtype: int64\n",
      "5. Number of females: 138\n",
      "6. Mean age: 41.63\n",
      "7. SD age: 13.70\n",
      "\n",
      "============================================================\n",
      "SUMMARY (for article):\n",
      "============================================================\n",
      "1. Total S1 N: 298\n",
      "2. Excluded based on implicit tasks: 33\n",
      "3. Additional exclusions (manipulation checks): 7\n",
      "4. Final N: 259\n",
      "5. Females: 138\n",
      "6. Mean age: 41.63\n",
      "7. SD age: 13.70\n"
     ]
    }
   ],
   "source": [
    "# 1. Total S1 N (before any exclusions)\n",
    "total_s1_n = len(df_s1)\n",
    "print(f\"1. Total S1 N: {total_s1_n}\")\n",
    "\n",
    "# Verify total excluded from exclude column\n",
    "exclude_col = pd.to_numeric(df_s1['exclude'], errors='coerce')\n",
    "if df_s1['exclude'].dtype == 'object':\n",
    "    exclude_col = df_s1['exclude'].astype(str).str.upper().isin(['TRUE', '1', 'YES']).astype(int)\n",
    "total_excluded = exclude_col.sum()\n",
    "print(f\"   Total excluded (from exclude column): {total_excluded}\")\n",
    "print(f\"   Should equal: {total_s1_n - len(df)} = {total_s1_n - len(df)}\")\n",
    "\n",
    "# 2. Exclusions based on implicit task performance\n",
    "# Check if exclude_t1 and exclude_t2 columns exist (from task files)\n",
    "# These indicate exclusion from T1 or T2 based on task performance\n",
    "excluded_from_tasks = pd.Series([False] * len(df_s1), index=df_s1.index)\n",
    "\n",
    "if 'exclude_t1' in df_s1.columns and 'exclude_t2' in df_s1.columns:\n",
    "    # Convert to boolean\n",
    "    exclude_t1_bool = pd.to_numeric(df_s1['exclude_t1'], errors='coerce')\n",
    "    exclude_t2_bool = pd.to_numeric(df_s1['exclude_t2'], errors='coerce')\n",
    "    \n",
    "    # Handle string/boolean values\n",
    "    if df_s1['exclude_t1'].dtype == 'object' or df_s1['exclude_t1'].dtype == 'bool':\n",
    "        exclude_t1_bool = df_s1['exclude_t1'].astype(str).str.upper().isin(['TRUE', '1', 'YES']).astype(int)\n",
    "    if df_s1['exclude_t2'].dtype == 'object' or df_s1['exclude_t2'].dtype == 'bool':\n",
    "        exclude_t2_bool = df_s1['exclude_t2'].astype(str).str.upper().isin(['TRUE', '1', 'YES']).astype(int)\n",
    "    \n",
    "    # Excluded from implicit tasks if excluded from T1 OR T2\n",
    "    excluded_from_tasks = ((exclude_t1_bool == 1) | (exclude_t2_bool == 1))\n",
    "    x1_task_exclusions = excluded_from_tasks.sum()\n",
    "    print(f\"\\n2. Excluded based on implicit task performance: {x1_task_exclusions}\")\n",
    "    print(f\"   (exclude_t1: {exclude_t1_bool.sum()}, exclude_t2: {exclude_t2_bool.sum()})\")\n",
    "elif 't1_exclude' in df_s1.columns and 't2_exclude' in df_s1.columns:\n",
    "    # Fallback to t1_exclude/t2_exclude if exclude_t1/exclude_t2 don't exist\n",
    "    t1_excluded = pd.to_numeric(df_s1['t1_exclude'], errors='coerce')\n",
    "    t2_excluded = pd.to_numeric(df_s1['t2_exclude'], errors='coerce')\n",
    "    \n",
    "    if df_s1['t1_exclude'].dtype == 'object':\n",
    "        t1_excluded = df_s1['t1_exclude'].astype(str).str.upper().isin(['TRUE', '1', 'YES']).astype(int)\n",
    "    if df_s1['t2_exclude'].dtype == 'object':\n",
    "        t2_excluded = df_s1['t2_exclude'].astype(str).str.upper().isin(['TRUE', '1', 'YES']).astype(int)\n",
    "    \n",
    "    excluded_from_tasks = ((t1_excluded == 1) | (t2_excluded == 1))\n",
    "    x1_task_exclusions = excluded_from_tasks.sum()\n",
    "    print(f\"\\n2. Excluded based on implicit task performance: {x1_task_exclusions}\")\n",
    "else:\n",
    "    print(\"\\n2. Warning: exclude_t1/exclude_t2 or t1_exclude/t2_exclude columns not found\")\n",
    "    x1_task_exclusions = 0\n",
    "\n",
    "# 3. Additional exclusions based on manipulation checks (only for manipulation groups, not control)\n",
    "# Manipulation groups are group 1 (Faking Low) and group 2 (Faking High)\n",
    "# Control group is group 0\n",
    "failed_manipulation = pd.Series([False] * len(df_s1), index=df_s1.index)\n",
    "\n",
    "if '1st check indicator' in df_s1.columns and '2nd check indicator' in df_s1.columns:\n",
    "    # Check what indicates wrong answer (likely \"*\" or similar)\n",
    "    manipulation_groups_mask = df_s1['group'].isin([1, 2])\n",
    "    manipulation_groups = df_s1[manipulation_groups_mask].copy()\n",
    "    \n",
    "    # Check for wrong answers in manipulation checks (look for \"*\" symbol)\n",
    "    wrong_1st = manipulation_groups['1st check indicator'].astype(str) == '*'\n",
    "    wrong_2nd = manipulation_groups['2nd check indicator'].astype(str) == '*'\n",
    "    \n",
    "    # Excluded if failed either check\n",
    "    failed_manipulation_subset = wrong_1st | wrong_2nd\n",
    "    x2_manipulation_exclusions = failed_manipulation_subset.sum()\n",
    "    \n",
    "    # Map back to full dataframe\n",
    "    failed_manipulation.loc[manipulation_groups.index] = failed_manipulation_subset.values\n",
    "    \n",
    "    print(f\"3. Additional exclusions from manipulation groups (failed checks): {x2_manipulation_exclusions}\")\n",
    "    print(f\"   (failed 1st check: {wrong_1st.sum()}, failed 2nd check: {wrong_2nd.sum()})\")\n",
    "    \n",
    "    # Check overlap: how many excluded for BOTH task performance AND manipulation checks\n",
    "    overlap = (excluded_from_tasks & failed_manipulation).sum()\n",
    "    if overlap > 0:\n",
    "        print(f\"   Note: {overlap} participants excluded for BOTH task performance AND manipulation checks\")\n",
    "    \n",
    "    # Verify: task exclusions + manipulation exclusions - overlap should match total (approximately)\n",
    "    # But note: some might be excluded for other reasons\n",
    "    calculated_total = x1_task_exclusions + x2_manipulation_exclusions - overlap\n",
    "    print(f\"   Calculated total (task + manipulation - overlap): {calculated_total}\")\n",
    "    print(f\"   Actual total excluded: {total_excluded}\")\n",
    "    if calculated_total != total_excluded:\n",
    "        other_exclusions = total_excluded - calculated_total\n",
    "        print(f\"   Difference (other exclusions?): {other_exclusions}\")\n",
    "else:\n",
    "    print(\"\\n3. Warning: manipulation check indicator columns not found\")\n",
    "    x2_manipulation_exclusions = 0\n",
    "    overlap = 0\n",
    "\n",
    "# 4. Final sample (after all exclusions)\n",
    "# Use the df dataframe which already has exclusions applied\n",
    "final_n = len(df)\n",
    "print(f\"\\n4. Final N (after all exclusions): {final_n}\")\n",
    "\n",
    "# 5. Demographics of final sample\n",
    "# Gender: check what values indicate female\n",
    "if 'gender' in df.columns:\n",
    "    # Check what values are in gender column\n",
    "    print(f\"\\nGender value counts:\")\n",
    "    print(df['gender'].value_counts())\n",
    "    \n",
    "    # Try to identify females (common values: 'Female', 'F', '2', etc.)\n",
    "    gender_str = df['gender'].astype(str).str.upper()\n",
    "    # Common female indicators\n",
    "    female_indicators = ['FEMALE', 'F', '2', 'WOMAN', 'W']\n",
    "    is_female = gender_str.isin(female_indicators)\n",
    "    n_females = is_female.sum()\n",
    "    print(f\"5. Number of females: {n_females}\")\n",
    "else:\n",
    "    print(\"\\n5. Warning: gender column not found\")\n",
    "    n_females = 0\n",
    "\n",
    "# Age statistics\n",
    "if 'age' in df.columns:\n",
    "    age_numeric = pd.to_numeric(df['age'], errors='coerce')\n",
    "    age_mean = age_numeric.mean()\n",
    "    age_sd = age_numeric.std(ddof=1)  # Sample SD\n",
    "    print(f\"6. Mean age: {age_mean:.2f}\")\n",
    "    print(f\"7. SD age: {age_sd:.2f}\")\n",
    "else:\n",
    "    print(\"\\n6-7. Warning: age column not found\")\n",
    "    age_mean = np.nan\n",
    "    age_sd = np.nan\n",
    "\n",
    "# Summary output for easy copy-paste\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"SUMMARY (for article):\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"1. Total S1 N: {total_s1_n}\")\n",
    "print(f\"2. Excluded based on implicit tasks: {x1_task_exclusions}\")\n",
    "print(f\"3. Additional exclusions (manipulation checks): {x2_manipulation_exclusions}\")\n",
    "print(f\"4. Final N: {final_n}\")\n",
    "print(f\"5. Females: {n_females}\")\n",
    "print(f\"6. Mean age: {age_mean:.2f}\")\n",
    "print(f\"7. SD age: {age_sd:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f418f1",
   "metadata": {},
   "source": [
    "## Internal Consistency of Questionnaire (Time 1)\n",
    "\n",
    "Compute Cronbach's alpha for the extraversion questionnaire at Time 1. Reverse-code items marked with \"*\" before computing reliability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3ae404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 Time 1 questionnaire columns:\n",
      "  Q1_A1\n",
      "  Q1_A10 *\n",
      "  Q1_A2 *\n",
      "  Q1_A3 *\n",
      "  Q1_A4\n",
      "  Q1_A5 *\n",
      "  Q1_A6\n",
      "  Q1_A7\n",
      "  Q1_A8 *\n",
      "  Q1_A9\n",
      "  Q1_A2 * - REVERSE CODED\n",
      "  Q1_A3 * - REVERSE CODED\n",
      "  Q1_A5 * - REVERSE CODED\n",
      "  Q1_A8 * - REVERSE CODED\n",
      "  Q1_A10 * - REVERSE CODED\n",
      "\n",
      "Reverse-coded items: 5\n",
      "Applied reverse coding to Q1_A2 *\n",
      "Applied reverse coding to Q1_A3 *\n",
      "Applied reverse coding to Q1_A5 *\n",
      "Applied reverse coding to Q1_A8 *\n",
      "Applied reverse coding to Q1_A10 *\n",
      "\n",
      "Participants with complete T1 questionnaire data: 259\n",
      "\n",
      "Cronbach's Alpha for T1 Questionnaire: 0.926\n",
      "Number of items: 10\n",
      "Sum of item variances: 12.700\n",
      "Total variance: 76.332\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get all Q1_A columns (Time 1 questionnaire items)\n",
    "q1_cols = [col for col in df.columns if col.startswith('Q1_A')]\n",
    "print(f\"Found {len(q1_cols)} Time 1 questionnaire columns:\")\n",
    "for col in sorted(q1_cols):\n",
    "    print(f\"  {col}\")\n",
    "\n",
    "# Based on the schema, reverse-coded items are those marked with \"*\" in the description\n",
    "# According to the schema: Q1_A2, Q1_A3, Q1_A5, Q1_A8, Q1_A10 are reverse-coded\n",
    "# Check if column names have \"*\" or use the known reverse-coded items\n",
    "reverse_coded_items = ['Q1_A2', 'Q1_A3', 'Q1_A5', 'Q1_A8', 'Q1_A10']\n",
    "reverse_coded = []\n",
    "\n",
    "for col in q1_cols:\n",
    "    # Check if column name contains \"*\" or matches known reverse-coded items\n",
    "    col_base = col.replace(' *', '').replace('*', '').strip()\n",
    "    if '*' in col or col.endswith(' *') or col_base in reverse_coded_items:\n",
    "        reverse_coded.append(col)\n",
    "        print(f\"  {col} - REVERSE CODED\")\n",
    "\n",
    "print(f\"\\nReverse-coded items: {len(reverse_coded)}\")\n",
    "if len(reverse_coded) == 0:\n",
    "    print(\"  Warning: No reverse-coded items found. Using schema-based list.\")\n",
    "    # Fallback: use items that exist in our list\n",
    "    reverse_coded = [col for col in q1_cols if any(item in col for item in reverse_coded_items)]\n",
    "    print(f\"  Using fallback: {reverse_coded}\")\n",
    "\n",
    "# Extract questionnaire data for Time 1\n",
    "q1_data = df[q1_cols].copy()\n",
    "\n",
    "# Convert to numeric\n",
    "for col in q1_cols:\n",
    "    q1_data[col] = pd.to_numeric(q1_data[col], errors='coerce')\n",
    "\n",
    "# Apply reverse coding: f(x) = 6 - x for reverse-coded items\n",
    "for col in reverse_coded:\n",
    "    if col in q1_data.columns:\n",
    "        # Reverse code: 6 - x\n",
    "        q1_data[col] = 6 - q1_data[col]\n",
    "        print(f\"Applied reverse coding to {col}\")\n",
    "\n",
    "# Remove rows with any missing values for reliability calculation\n",
    "q1_data_clean = q1_data.dropna()\n",
    "print(f\"\\nParticipants with complete T1 questionnaire data: {len(q1_data_clean)}\")\n",
    "\n",
    "# Compute Cronbach's alpha\n",
    "# Formula: α = (k / (k-1)) * (1 - Σσ²ᵢ / σ²ₜ)\n",
    "# where k = number of items, σ²ᵢ = variance of item i, σ²ₜ = total variance\n",
    "k = len(q1_cols)\n",
    "item_variances = q1_data_clean.var(axis=0, ddof=1)\n",
    "total_variance = q1_data_clean.sum(axis=1).var(ddof=1)\n",
    "\n",
    "cronbach_alpha = (k / (k - 1)) * (1 - item_variances.sum() / total_variance)\n",
    "\n",
    "print(f\"\\nCronbach's Alpha for T1 Questionnaire: {cronbach_alpha:.3f}\")\n",
    "print(f\"Number of items: {k}\")\n",
    "print(f\"Sum of item variances: {item_variances.sum():.3f}\")\n",
    "print(f\"Total variance: {total_variance:.3f}\")\n",
    "\n",
    "# Store for later use\n",
    "t1_questionnaire_alpha = cronbach_alpha\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e108b0",
   "metadata": {},
   "source": [
    "## Split-Half Reliability (Time 1)\n",
    "\n",
    "Compute split-half reliabilities for qIAT and IAT at Time 1 using paired-odd and paired-even D-scores, with Spearman-Brown correction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a99b2420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qIAT Time 1 participants with both odd and even D-scores: 119\n",
      "qIAT T1 correlation (odd vs even): r = 0.916, p = 0.0000\n",
      "qIAT T1 split-half reliability (Spearman-Brown corrected): 0.956\n",
      "\n",
      "------------------------------------------------------------\n",
      "IAT Time 1 participants with both odd and even D-scores: 140\n",
      "IAT T1 correlation (odd vs even): r = 0.829, p = 0.0000\n",
      "IAT T1 split-half reliability (Spearman-Brown corrected): 0.907\n",
      "\n",
      "============================================================\n",
      "SUMMARY (for article):\n",
      "============================================================\n",
      "qIAT split-half reliability (T1): 0.96\n",
      "IAT split-half reliability (T1): 0.91\n"
     ]
    }
   ],
   "source": [
    "# Get Time 1 data for qIAT and IAT\n",
    "# Use dscore_paired_odd_t1 and dscore_paired_even_t1 columns from the combined file\n",
    "\n",
    "# For qIAT\n",
    "qiat_t1 = df[df['task'].str.upper() == 'QIAT'].copy()\n",
    "qiat_t1_odd = pd.to_numeric(qiat_t1['dscore_paired_odd_t1'], errors='coerce')\n",
    "qiat_t1_even = pd.to_numeric(qiat_t1['dscore_paired_even_t1'], errors='coerce')\n",
    "\n",
    "# Remove missing values\n",
    "qiat_t1_pairs = pd.DataFrame({\n",
    "    'odd': qiat_t1_odd,\n",
    "    'even': qiat_t1_even\n",
    "}).dropna()\n",
    "\n",
    "print(f\"qIAT Time 1 participants with both odd and even D-scores: {len(qiat_t1_pairs)}\")\n",
    "\n",
    "if len(qiat_t1_pairs) > 0:\n",
    "    # Compute correlation\n",
    "    qiat_t1_corr, qiat_t1_p = pearsonr(qiat_t1_pairs['odd'], qiat_t1_pairs['even'])\n",
    "    print(f\"qIAT T1 correlation (odd vs even): r = {qiat_t1_corr:.3f}, p = {qiat_t1_p:.4f}\")\n",
    "    \n",
    "    # Apply Spearman-Brown correction\n",
    "    qiat_t1_reliability = spearman_brown(qiat_t1_corr)\n",
    "    print(f\"qIAT T1 split-half reliability (Spearman-Brown corrected): {qiat_t1_reliability:.3f}\")\n",
    "else:\n",
    "    print(\"Warning: No valid qIAT T1 data found\")\n",
    "    qiat_t1_reliability = np.nan\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "\n",
    "# For IAT\n",
    "iat_t1 = df[df['task'].str.upper() == 'IAT'].copy()\n",
    "iat_t1_odd = pd.to_numeric(iat_t1['dscore_paired_odd_t1'], errors='coerce')\n",
    "iat_t1_even = pd.to_numeric(iat_t1['dscore_paired_even_t1'], errors='coerce')\n",
    "\n",
    "# Remove missing values\n",
    "iat_t1_pairs = pd.DataFrame({\n",
    "    'odd': iat_t1_odd,\n",
    "    'even': iat_t1_even\n",
    "}).dropna()\n",
    "\n",
    "print(f\"IAT Time 1 participants with both odd and even D-scores: {len(iat_t1_pairs)}\")\n",
    "\n",
    "if len(iat_t1_pairs) > 0:\n",
    "    # Compute correlation\n",
    "    iat_t1_corr, iat_t1_p = pearsonr(iat_t1_pairs['odd'], iat_t1_pairs['even'])\n",
    "    print(f\"IAT T1 correlation (odd vs even): r = {iat_t1_corr:.3f}, p = {iat_t1_p:.4f}\")\n",
    "    \n",
    "    # Apply Spearman-Brown correction\n",
    "    iat_t1_reliability = spearman_brown(iat_t1_corr)\n",
    "    print(f\"IAT T1 split-half reliability (Spearman-Brown corrected): {iat_t1_reliability:.3f}\")\n",
    "else:\n",
    "    print(\"Warning: No valid IAT T1 data found\")\n",
    "    iat_t1_reliability = np.nan\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY (for article):\")\n",
    "print(\"=\"*60)\n",
    "print(f\"qIAT split-half reliability (T1): {qiat_t1_reliability:.2f}\")\n",
    "print(f\"IAT split-half reliability (T1): {iat_t1_reliability:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f07964",
   "metadata": {},
   "source": [
    "## Table 3: Descriptive Statistics (Means and SDs)\n",
    "\n",
    "Table 3 showing means (SDs) for questionnaire, IAT, and qIAT by group (Control, Faking Low, Faking High) and time (Time1, Time2).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5229e016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 3. Means (SDs) for the questionnaire and implicit tasks in Study 1\n",
      "========================================================================================================================\n",
      "Measure         Control                        Faking Low                     Faking High                    Overall                       \n",
      "                Time1          Time2          Time1          Time2          Time1          Time2          Time1          Time2         \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Questionnaire  28.07 (8.75)   28.72 (9.17)  26.61 (8.16)   12.54 (5.14)  29.04 (9.20)   48.35 (4.28)  27.91 (8.74)   29.85 (15.97) \n",
      "IAT            -0.48 (0.57)   -0.34 (0.43)  -0.41 (0.61)   -0.35 (0.44)  -0.47 (0.56)   -0.25 (0.53)  -0.45 (0.58)   -0.31 (0.47)  \n",
      "qIAT           0.31 (0.79)    0.35 (0.68)   -0.01 (0.85)   -0.05 (0.64)  0.21 (1.02)    0.42 (0.63)   0.17 (0.89)    0.25 (0.67)   \n",
      "========================================================================================================================\n",
      "\n",
      "Detailed table (DataFrame format):\n",
      "      Measure       Group  Time1_Mean  Time1_SD  Time2_Mean  Time2_SD Time1_Formatted Time2_Formatted\n",
      "Questionnaire     Control   28.067416  8.747464   28.719101  9.166377    28.07 (8.75)    28.72 (9.17)\n",
      "Questionnaire  Faking Low   26.611765  8.159269   12.541176  5.135203    26.61 (8.16)    12.54 (5.14)\n",
      "Questionnaire Faking High   29.035294  9.203321   48.352941  4.283706    29.04 (9.20)    48.35 (4.28)\n",
      "Questionnaire     Overall   27.907336  8.736845   29.853282 15.973625    27.91 (8.74)   29.85 (15.97)\n",
      "          IAT     Control   -0.484752  0.566560   -0.337021  0.426006    -0.48 (0.57)    -0.34 (0.43)\n",
      "          IAT  Faking Low   -0.405946  0.611913   -0.352670  0.444794    -0.41 (0.61)    -0.35 (0.44)\n",
      "          IAT Faking High   -0.471236  0.556230   -0.245464  0.532645    -0.47 (0.56)    -0.25 (0.53)\n",
      "          IAT     Overall   -0.453855  0.575763   -0.312192  0.468632    -0.45 (0.58)    -0.31 (0.47)\n",
      "         qIAT     Control    0.305790  0.792398    0.351558  0.675286     0.31 (0.79)     0.35 (0.68)\n",
      "         qIAT  Faking Low   -0.010716  0.852622   -0.046186  0.638966    -0.01 (0.85)    -0.05 (0.64)\n",
      "         qIAT Faking High    0.211366  1.018553    0.417286  0.626933     0.21 (1.02)     0.42 (0.63)\n",
      "         qIAT     Overall    0.173775  0.893218    0.246088  0.673939     0.17 (0.89)     0.25 (0.67)\n",
      "\n",
      "\n",
      "Pivot table format:\n",
      "              Time1_Formatted                                            \\\n",
      "Group                 Control   Faking High    Faking Low       Overall   \n",
      "Measure                                                                   \n",
      "IAT              -0.48 (0.57)  -0.47 (0.56)  -0.41 (0.61)  -0.45 (0.58)   \n",
      "Questionnaire    28.07 (8.75)  29.04 (9.20)  26.61 (8.16)  27.91 (8.74)   \n",
      "qIAT              0.31 (0.79)   0.21 (1.02)  -0.01 (0.85)   0.17 (0.89)   \n",
      "\n",
      "              Time2_Formatted                                             \n",
      "Group                 Control   Faking High    Faking Low        Overall  \n",
      "Measure                                                                   \n",
      "IAT              -0.34 (0.43)  -0.25 (0.53)  -0.35 (0.44)   -0.31 (0.47)  \n",
      "Questionnaire    28.72 (9.17)  48.35 (4.28)  12.54 (5.14)  29.85 (15.97)  \n",
      "qIAT              0.35 (0.68)   0.42 (0.63)  -0.05 (0.64)    0.25 (0.67)  \n"
     ]
    }
   ],
   "source": [
    "# Group mapping: 0 = Control, 1 = Faking Low, 2 = Faking High\n",
    "group_names = {0: 'Control', 1: 'Faking Low', 2: 'Faking High'}\n",
    "\n",
    "# Initialize results dictionary\n",
    "results = {\n",
    "    'Questionnaire': {},\n",
    "    'IAT': {},\n",
    "    'qIAT': {}\n",
    "}\n",
    "\n",
    "# 1. Questionnaire (use t1_ques and t2_ques)\n",
    "if 't1_ques' in df.columns and 't2_ques' in df.columns:\n",
    "    for group_num, group_name in group_names.items():\n",
    "        group_data = df[df['group'] == group_num]\n",
    "        \n",
    "        t1_ques = pd.to_numeric(group_data['t1_ques'], errors='coerce')\n",
    "        t2_ques = pd.to_numeric(group_data['t2_ques'], errors='coerce')\n",
    "        \n",
    "        t1_mean = t1_ques.mean()\n",
    "        t1_sd = t1_ques.std(ddof=1)\n",
    "        t2_mean = t2_ques.mean()\n",
    "        t2_sd = t2_ques.std(ddof=1)\n",
    "        \n",
    "        results['Questionnaire'][group_name] = {\n",
    "            'Time1': (t1_mean, t1_sd),\n",
    "            'Time2': (t2_mean, t2_sd)\n",
    "        }\n",
    "    \n",
    "    # Overall (all groups combined)\n",
    "    t1_ques_all = pd.to_numeric(df['t1_ques'], errors='coerce')\n",
    "    t2_ques_all = pd.to_numeric(df['t2_ques'], errors='coerce')\n",
    "    results['Questionnaire']['Overall'] = {\n",
    "        'Time1': (t1_ques_all.mean(), t1_ques_all.std(ddof=1)),\n",
    "        'Time2': (t2_ques_all.mean(), t2_ques_all.std(ddof=1))\n",
    "    }\n",
    "\n",
    "# 2. IAT (filter task == 'IAT', use t1_dscore and t2_dscore)\n",
    "iat_data = df[df['task'].str.upper() == 'IAT'].copy()\n",
    "if len(iat_data) > 0:\n",
    "    for group_num, group_name in group_names.items():\n",
    "        group_data = iat_data[iat_data['group'] == group_num]\n",
    "        \n",
    "        t1_dscore = pd.to_numeric(group_data['t1_dscore'], errors='coerce')\n",
    "        t2_dscore = pd.to_numeric(group_data['t2_dscore'], errors='coerce')\n",
    "        \n",
    "        t1_mean = t1_dscore.mean()\n",
    "        t1_sd = t1_dscore.std(ddof=1)\n",
    "        t2_mean = t2_dscore.mean()\n",
    "        t2_sd = t2_dscore.std(ddof=1)\n",
    "        \n",
    "        results['IAT'][group_name] = {\n",
    "            'Time1': (t1_mean, t1_sd),\n",
    "            'Time2': (t2_mean, t2_sd)\n",
    "        }\n",
    "    \n",
    "    # Overall (all groups combined)\n",
    "    t1_dscore_all = pd.to_numeric(iat_data['t1_dscore'], errors='coerce')\n",
    "    t2_dscore_all = pd.to_numeric(iat_data['t2_dscore'], errors='coerce')\n",
    "    results['IAT']['Overall'] = {\n",
    "        'Time1': (t1_dscore_all.mean(), t1_dscore_all.std(ddof=1)),\n",
    "        'Time2': (t2_dscore_all.mean(), t2_dscore_all.std(ddof=1))\n",
    "    }\n",
    "\n",
    "# 3. qIAT (filter task == 'qIAT', use t1_dscore and t2_dscore)\n",
    "qiat_data = df[df['task'].str.upper() == 'QIAT'].copy()\n",
    "if len(qiat_data) > 0:\n",
    "    for group_num, group_name in group_names.items():\n",
    "        group_data = qiat_data[qiat_data['group'] == group_num]\n",
    "        \n",
    "        t1_dscore = pd.to_numeric(group_data['t1_dscore'], errors='coerce')\n",
    "        t2_dscore = pd.to_numeric(group_data['t2_dscore'], errors='coerce')\n",
    "        \n",
    "        t1_mean = t1_dscore.mean()\n",
    "        t1_sd = t1_dscore.std(ddof=1)\n",
    "        t2_mean = t2_dscore.mean()\n",
    "        t2_sd = t2_dscore.std(ddof=1)\n",
    "        \n",
    "        results['qIAT'][group_name] = {\n",
    "            'Time1': (t1_mean, t1_sd),\n",
    "            'Time2': (t2_mean, t2_sd)\n",
    "        }\n",
    "    \n",
    "    # Overall (all groups combined)\n",
    "    t1_dscore_all = pd.to_numeric(qiat_data['t1_dscore'], errors='coerce')\n",
    "    t2_dscore_all = pd.to_numeric(qiat_data['t2_dscore'], errors='coerce')\n",
    "    results['qIAT']['Overall'] = {\n",
    "        'Time1': (t1_dscore_all.mean(), t1_dscore_all.std(ddof=1)),\n",
    "        'Time2': (t2_dscore_all.mean(), t2_dscore_all.std(ddof=1))\n",
    "    }\n",
    "\n",
    "# Create formatted table\n",
    "print(\"Table 3. Means (SDs) for the questionnaire and implicit tasks in Study 1\")\n",
    "print(\"=\" * 120)\n",
    "header1 = f\"{'Measure':<15} {'Control':<30} {'Faking Low':<30} {'Faking High':<30} {'Overall':<30}\"\n",
    "print(header1)\n",
    "header2 = f\"{'':<15} {'Time1':<14} {'Time2':<14} {'Time1':<14} {'Time2':<14} {'Time1':<14} {'Time2':<14} {'Time1':<14} {'Time2':<14}\"\n",
    "print(header2)\n",
    "print(\"-\" * 120)\n",
    "\n",
    "for measure in ['Questionnaire', 'IAT', 'qIAT']:\n",
    "    row = f\"{measure:<15}\"\n",
    "    \n",
    "    for group_name in ['Control', 'Faking Low', 'Faking High', 'Overall']:\n",
    "        if group_name in results[measure]:\n",
    "            t1_mean, t1_sd = results[measure][group_name]['Time1']\n",
    "            t2_mean, t2_sd = results[measure][group_name]['Time2']\n",
    "            t1_str = f\"{t1_mean:.2f} ({t1_sd:.2f})\"\n",
    "            t2_str = f\"{t2_mean:.2f} ({t2_sd:.2f})\"\n",
    "            row += f\"{t1_str:<14} {t2_str:<14}\"\n",
    "        else:\n",
    "            row += f\"{'N/A':<14} {'N/A':<14}\"\n",
    "    \n",
    "    print(row)\n",
    "\n",
    "print(\"=\" * 120)\n",
    "\n",
    "# Also create a pandas DataFrame for easier viewing\n",
    "table_data = []\n",
    "for measure in ['Questionnaire', 'IAT', 'qIAT']:\n",
    "    for group_name in ['Control', 'Faking Low', 'Faking High', 'Overall']:\n",
    "        if group_name in results[measure]:\n",
    "            t1_mean, t1_sd = results[measure][group_name]['Time1']\n",
    "            t2_mean, t2_sd = results[measure][group_name]['Time2']\n",
    "            table_data.append({\n",
    "                'Measure': measure,\n",
    "                'Group': group_name,\n",
    "                'Time1_Mean': t1_mean,\n",
    "                'Time1_SD': t1_sd,\n",
    "                'Time2_Mean': t2_mean,\n",
    "                'Time2_SD': t2_sd,\n",
    "                'Time1_Formatted': f\"{t1_mean:.2f} ({t1_sd:.2f})\",\n",
    "                'Time2_Formatted': f\"{t2_mean:.2f} ({t2_sd:.2f})\"\n",
    "            })\n",
    "\n",
    "table_df = pd.DataFrame(table_data)\n",
    "print(\"\\nDetailed table (DataFrame format):\")\n",
    "print(table_df.to_string(index=False))\n",
    "\n",
    "# Create a pivot table for better visualization\n",
    "pivot_table = table_df.pivot(index='Measure', columns='Group', values=['Time1_Formatted', 'Time2_Formatted'])\n",
    "print(\"\\n\\nPivot table format:\")\n",
    "print(pivot_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22285758",
   "metadata": {},
   "source": [
    "## Test-Retest Reliability (Control Group)\n",
    "\n",
    "Compute test-retest reliability for self-report, qIAT, and IAT by correlating Time1 and Time2 scores in the control group only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e87d5ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Control group participants: 89\n",
      "\n",
      "Self-report (Questionnaire):\n",
      "  Participants with complete data: 89\n",
      "  Test-retest reliability: r = 0.962, p = 0.0000\n",
      "\n",
      "qIAT (Control group):\n",
      "  Participants: 42\n",
      "  Participants with complete data: 42\n",
      "  Test-retest reliability: r = 0.727, p = 0.0000\n",
      "\n",
      "IAT (Control group):\n",
      "  Participants: 47\n",
      "  Participants with complete data: 47\n",
      "  Test-retest reliability: r = 0.508, p = 0.0003\n",
      "\n",
      "============================================================\n",
      "SUMMARY (for article):\n",
      "============================================================\n",
      "Test-retest reliability (Control group):\n",
      "  Self-report: 0.96\n",
      "  qIAT: 0.73\n",
      "  IAT: 0.51\n"
     ]
    }
   ],
   "source": [
    "# Filter to control group only (group == 0)\n",
    "control_group = df[df['group'] == 0].copy()\n",
    "print(f\"Control group participants: {len(control_group)}\")\n",
    "\n",
    "# 1. Self-report (Questionnaire) test-retest reliability\n",
    "if 't1_ques' in control_group.columns and 't2_ques' in control_group.columns:\n",
    "    t1_ques_control = pd.to_numeric(control_group['t1_ques'], errors='coerce')\n",
    "    t2_ques_control = pd.to_numeric(control_group['t2_ques'], errors='coerce')\n",
    "    \n",
    "    # Remove missing values\n",
    "    ques_pairs = pd.DataFrame({\n",
    "        't1': t1_ques_control,\n",
    "        't2': t2_ques_control\n",
    "    }).dropna()\n",
    "    \n",
    "    print(f\"\\nSelf-report (Questionnaire):\")\n",
    "    print(f\"  Participants with complete data: {len(ques_pairs)}\")\n",
    "    \n",
    "    if len(ques_pairs) > 1:\n",
    "        ques_corr, ques_p = pearsonr(ques_pairs['t1'], ques_pairs['t2'])\n",
    "        print(f\"  Test-retest reliability: r = {ques_corr:.3f}, p = {ques_p:.4f}\")\n",
    "        self_report_reliability = ques_corr\n",
    "    else:\n",
    "        print(\"  Warning: Insufficient data for correlation\")\n",
    "        self_report_reliability = np.nan\n",
    "else:\n",
    "    print(\"\\nWarning: t1_ques or t2_ques columns not found\")\n",
    "    self_report_reliability = np.nan\n",
    "\n",
    "# 2. qIAT test-retest reliability (control group, qIAT task only)\n",
    "qiat_control = control_group[control_group['task'].str.upper() == 'QIAT'].copy()\n",
    "print(f\"\\nqIAT (Control group):\")\n",
    "print(f\"  Participants: {len(qiat_control)}\")\n",
    "\n",
    "if len(qiat_control) > 0:\n",
    "    t1_qiat_control = pd.to_numeric(qiat_control['t1_dscore'], errors='coerce')\n",
    "    t2_qiat_control = pd.to_numeric(qiat_control['t2_dscore'], errors='coerce')\n",
    "    \n",
    "    # Remove missing values\n",
    "    qiat_pairs = pd.DataFrame({\n",
    "        't1': t1_qiat_control,\n",
    "        't2': t2_qiat_control\n",
    "    }).dropna()\n",
    "    \n",
    "    print(f\"  Participants with complete data: {len(qiat_pairs)}\")\n",
    "    \n",
    "    if len(qiat_pairs) > 1:\n",
    "        qiat_corr, qiat_p = pearsonr(qiat_pairs['t1'], qiat_pairs['t2'])\n",
    "        print(f\"  Test-retest reliability: r = {qiat_corr:.3f}, p = {qiat_p:.4f}\")\n",
    "        qiat_reliability = qiat_corr\n",
    "    else:\n",
    "        print(\"  Warning: Insufficient data for correlation\")\n",
    "        qiat_reliability = np.nan\n",
    "else:\n",
    "    print(\"  Warning: No qIAT data found for control group\")\n",
    "    qiat_reliability = np.nan\n",
    "\n",
    "# 3. IAT test-retest reliability (control group, IAT task only)\n",
    "iat_control = control_group[control_group['task'].str.upper() == 'IAT'].copy()\n",
    "print(f\"\\nIAT (Control group):\")\n",
    "print(f\"  Participants: {len(iat_control)}\")\n",
    "\n",
    "if len(iat_control) > 0:\n",
    "    t1_iat_control = pd.to_numeric(iat_control['t1_dscore'], errors='coerce')\n",
    "    t2_iat_control = pd.to_numeric(iat_control['t2_dscore'], errors='coerce')\n",
    "    \n",
    "    # Remove missing values\n",
    "    iat_pairs = pd.DataFrame({\n",
    "        't1': t1_iat_control,\n",
    "        't2': t2_iat_control\n",
    "    }).dropna()\n",
    "    \n",
    "    print(f\"  Participants with complete data: {len(iat_pairs)}\")\n",
    "    \n",
    "    if len(iat_pairs) > 1:\n",
    "        iat_corr, iat_p = pearsonr(iat_pairs['t1'], iat_pairs['t2'])\n",
    "        print(f\"  Test-retest reliability: r = {iat_corr:.3f}, p = {iat_p:.4f}\")\n",
    "        iat_reliability = iat_corr\n",
    "    else:\n",
    "        print(\"  Warning: Insufficient data for correlation\")\n",
    "        iat_reliability = np.nan\n",
    "else:\n",
    "    print(\"  Warning: No IAT data found for control group\")\n",
    "    iat_reliability = np.nan\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY (for article):\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Test-retest reliability (Control group):\")\n",
    "print(f\"  Self-report: {self_report_reliability:.2f}\")\n",
    "print(f\"  qIAT: {qiat_reliability:.2f}\")\n",
    "print(f\"  IAT: {iat_reliability:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487ac26e",
   "metadata": {},
   "source": [
    "## Table 4: Split-Half Reliabilities\n",
    "\n",
    "Table 4 showing split-half reliabilities for IAT and qIAT across groups (Control, Faking Low, Faking High, Overall) and time points (Time1, Time2). Uses paired-odd and paired-even D-scores with Spearman-Brown correction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1e1fe6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 4. Split half reliabilities of implicit tasks in Study 1\n",
      "========================================================================================================================\n",
      "Measure    Control                        Faking Low                     Faking High                    Overall                       \n",
      "           Time1          Time2          Time1          Time2          Time1          Time2          Time1          Time2         \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "IAT       0.92***        0.88***       0.89***        0.86***       0.91***        0.87***       0.91***        0.87***       \n",
      "qIAT      0.95***        0.93***       0.96***        0.90***       0.96***        0.91***       0.96***        0.92***       \n",
      "========================================================================================================================\n",
      "Note: * p < .05, ** p < .01, *** p < .001\n",
      "\n",
      "\n",
      "Detailed table (DataFrame format):\n",
      "Task       Group  Time1_Reliability      Time1_p Time1_Formatted  Time2_Reliability      Time2_p Time2_Formatted\n",
      " IAT     Control           0.923810 1.230332e-14         0.92***           0.875651 1.150548e-10         0.88***\n",
      " IAT  Faking Low           0.888082 1.732955e-11         0.89***           0.864630 5.118953e-10         0.86***\n",
      " IAT Faking High           0.909825 5.749101e-13         0.91***           0.867901 5.259895e-10         0.87***\n",
      " IAT     Overall           0.906748 1.038243e-36         0.91***           0.866265 4.746801e-28         0.87***\n",
      "qIAT     Control           0.952911 6.873562e-17         0.95***           0.930738 6.964443e-14         0.93***\n",
      "qIAT  Faking Low           0.960993 1.076726e-16         0.96***           0.898015 4.747465e-10         0.90***\n",
      "qIAT Faking High           0.958294 1.271393e-16         0.96***           0.914396 1.796285e-11         0.91***\n",
      "qIAT     Overall           0.956344 2.488309e-48         0.96***           0.920214 1.041768e-34         0.92***\n"
     ]
    }
   ],
   "source": [
    "# Helper function to add significance asterisks\n",
    "def add_significance(p_value):\n",
    "    \"\"\"Add significance asterisks based on p-value.\"\"\"\n",
    "    if p_value < 0.001:\n",
    "        return \"***\"\n",
    "    elif p_value < 0.01:\n",
    "        return \"**\"\n",
    "    elif p_value < 0.05:\n",
    "        return \"*\"\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "# Group mapping: 0 = Control, 1 = Faking Low, 2 = Faking High\n",
    "group_names = {0: 'Control', 1: 'Faking Low', 2: 'Faking High'}\n",
    "\n",
    "# Initialize results dictionary\n",
    "# Structure: results[task][group][time] = (reliability, p_value, formatted_string)\n",
    "results = {\n",
    "    'IAT': {},\n",
    "    'qIAT': {}\n",
    "}\n",
    "\n",
    "# Function to compute split-half reliability for a subset of data\n",
    "def compute_split_half_reliability(data, odd_col, even_col):\n",
    "    \"\"\"Compute split-half reliability from odd and even D-scores.\"\"\"\n",
    "    odd_scores = pd.to_numeric(data[odd_col], errors='coerce')\n",
    "    even_scores = pd.to_numeric(data[even_col], errors='coerce')\n",
    "    \n",
    "    # Remove missing values\n",
    "    pairs = pd.DataFrame({\n",
    "        'odd': odd_scores,\n",
    "        'even': even_scores\n",
    "    }).dropna()\n",
    "    \n",
    "    if len(pairs) < 2:\n",
    "        return np.nan, np.nan, \"N/A\"\n",
    "    \n",
    "    # Compute correlation\n",
    "    corr, p_value = pearsonr(pairs['odd'], pairs['even'])\n",
    "    \n",
    "    # Apply Spearman-Brown correction\n",
    "    reliability = spearman_brown(corr)\n",
    "    \n",
    "    # Format with significance\n",
    "    sig = add_significance(p_value)\n",
    "    formatted = f\"{reliability:.2f}{sig}\"\n",
    "    \n",
    "    return reliability, p_value, formatted\n",
    "\n",
    "# Process IAT\n",
    "iat_data = df[df['task'].str.upper() == 'IAT'].copy()\n",
    "\n",
    "for group_num, group_name in group_names.items():\n",
    "    group_iat = iat_data[iat_data['group'] == group_num].copy()\n",
    "    \n",
    "    # Time 1\n",
    "    rel_t1, p_t1, fmt_t1 = compute_split_half_reliability(\n",
    "        group_iat, 'dscore_paired_odd_t1', 'dscore_paired_even_t1'\n",
    "    )\n",
    "    \n",
    "    # Time 2\n",
    "    rel_t2, p_t2, fmt_t2 = compute_split_half_reliability(\n",
    "        group_iat, 'dscore_paired_odd_t2', 'dscore_paired_even_t2'\n",
    "    )\n",
    "    \n",
    "    results['IAT'][group_name] = {\n",
    "        'Time1': (rel_t1, p_t1, fmt_t1),\n",
    "        'Time2': (rel_t2, p_t2, fmt_t2)\n",
    "    }\n",
    "\n",
    "# IAT Overall\n",
    "rel_t1_all, p_t1_all, fmt_t1_all = compute_split_half_reliability(\n",
    "    iat_data, 'dscore_paired_odd_t1', 'dscore_paired_even_t1'\n",
    ")\n",
    "rel_t2_all, p_t2_all, fmt_t2_all = compute_split_half_reliability(\n",
    "    iat_data, 'dscore_paired_odd_t2', 'dscore_paired_even_t2'\n",
    ")\n",
    "results['IAT']['Overall'] = {\n",
    "    'Time1': (rel_t1_all, p_t1_all, fmt_t1_all),\n",
    "    'Time2': (rel_t2_all, p_t2_all, fmt_t2_all)\n",
    "}\n",
    "\n",
    "# Process qIAT\n",
    "qiat_data = df[df['task'].str.upper() == 'QIAT'].copy()\n",
    "\n",
    "for group_num, group_name in group_names.items():\n",
    "    group_qiat = qiat_data[qiat_data['group'] == group_num].copy()\n",
    "    \n",
    "    # Time 1\n",
    "    rel_t1, p_t1, fmt_t1 = compute_split_half_reliability(\n",
    "        group_qiat, 'dscore_paired_odd_t1', 'dscore_paired_even_t1'\n",
    "    )\n",
    "    \n",
    "    # Time 2\n",
    "    rel_t2, p_t2, fmt_t2 = compute_split_half_reliability(\n",
    "        group_qiat, 'dscore_paired_odd_t2', 'dscore_paired_even_t2'\n",
    "    )\n",
    "    \n",
    "    results['qIAT'][group_name] = {\n",
    "        'Time1': (rel_t1, p_t1, fmt_t1),\n",
    "        'Time2': (rel_t2, p_t2, fmt_t2)\n",
    "    }\n",
    "\n",
    "# qIAT Overall\n",
    "rel_t1_all, p_t1_all, fmt_t1_all = compute_split_half_reliability(\n",
    "    qiat_data, 'dscore_paired_odd_t1', 'dscore_paired_even_t1'\n",
    ")\n",
    "rel_t2_all, p_t2_all, fmt_t2_all = compute_split_half_reliability(\n",
    "    qiat_data, 'dscore_paired_odd_t2', 'dscore_paired_even_t2'\n",
    ")\n",
    "results['qIAT']['Overall'] = {\n",
    "    'Time1': (rel_t1_all, p_t1_all, fmt_t1_all),\n",
    "    'Time2': (rel_t2_all, p_t2_all, fmt_t2_all)\n",
    "}\n",
    "\n",
    "# Create formatted table\n",
    "print(\"Table 4. Split half reliabilities of implicit tasks in Study 1\")\n",
    "print(\"=\" * 120)\n",
    "header1 = f\"{'Measure':<10} {'Control':<30} {'Faking Low':<30} {'Faking High':<30} {'Overall':<30}\"\n",
    "print(header1)\n",
    "header2 = f\"{'':<10} {'Time1':<14} {'Time2':<14} {'Time1':<14} {'Time2':<14} {'Time1':<14} {'Time2':<14} {'Time1':<14} {'Time2':<14}\"\n",
    "print(header2)\n",
    "print(\"-\" * 120)\n",
    "\n",
    "for task in ['IAT', 'qIAT']:\n",
    "    row = f\"{task:<10}\"\n",
    "    \n",
    "    for group_name in ['Control', 'Faking Low', 'Faking High', 'Overall']:\n",
    "        if group_name in results[task]:\n",
    "            fmt_t1 = results[task][group_name]['Time1'][2]\n",
    "            fmt_t2 = results[task][group_name]['Time2'][2]\n",
    "            row += f\"{fmt_t1:<14} {fmt_t2:<14}\"\n",
    "        else:\n",
    "            row += f\"{'N/A':<14} {'N/A':<14}\"\n",
    "    \n",
    "    print(row)\n",
    "\n",
    "print(\"=\" * 120)\n",
    "print(\"Note: * p < .05, ** p < .01, *** p < .001\")\n",
    "\n",
    "# Create detailed DataFrame with all information\n",
    "table_data = []\n",
    "for task in ['IAT', 'qIAT']:\n",
    "    for group_name in ['Control', 'Faking Low', 'Faking High', 'Overall']:\n",
    "        if group_name in results[task]:\n",
    "            rel_t1, p_t1, fmt_t1 = results[task][group_name]['Time1']\n",
    "            rel_t2, p_t2, fmt_t2 = results[task][group_name]['Time2']\n",
    "            table_data.append({\n",
    "                'Task': task,\n",
    "                'Group': group_name,\n",
    "                'Time1_Reliability': rel_t1,\n",
    "                'Time1_p': p_t1,\n",
    "                'Time1_Formatted': fmt_t1,\n",
    "                'Time2_Reliability': rel_t2,\n",
    "                'Time2_p': p_t2,\n",
    "                'Time2_Formatted': fmt_t2\n",
    "            })\n",
    "\n",
    "table_df = pd.DataFrame(table_data)\n",
    "print(\"\\n\\nDetailed table (DataFrame format):\")\n",
    "print(table_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cb2eaa",
   "metadata": {},
   "source": [
    "## Table 5: Implicit-Explicit Correlations\n",
    "\n",
    "Compute correlations between implicit tasks (IAT and qIAT) and the explicit questionnaire measure at Time1 and Time2, separately for each group (Control, Faking Low, Faking High, Overall).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6bdbe4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 5. Correlations between implicit tasks and the explicit measure in Study 1\n",
      "========================================================================================================================\n",
      "Measure    Control                        Faking Low                     Faking High                    Overall                       \n",
      "           Time1          Time2          Time1          Time2          Time1          Time2          Time1          Time2         \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "IAT       0.27           0.42**        0.49***        0.07          0.45**         0.08          0.39***        0.17*         \n",
      "qIAT      0.55***        0.67***       0.53***        0.09          0.72***        0.28          0.62***        0.43***       \n",
      "========================================================================================================================\n",
      "Note: * p < .05, ** p < .01, *** p < .001\n",
      "\n",
      "\n",
      "Detailed table (DataFrame format):\n",
      "Task       Group  Time1_Correlation      Time1_p Time1_Formatted  Time2_Correlation  Time2_p Time2_Formatted\n",
      " IAT     Control           0.270195 6.623768e-02            0.27           0.419726 0.003314          0.42**\n",
      " IAT  Faking Low           0.490696 4.622290e-04         0.49***           0.070774 0.636404            0.07\n",
      " IAT Faking High           0.451677 1.627571e-03          0.45**           0.081866 0.588594            0.08\n",
      " IAT     Overall           0.391140 1.767696e-06         0.39***           0.169824 0.044861           0.17*\n",
      "qIAT     Control           0.545307 1.881107e-04         0.55***           0.667875 0.000001         0.67***\n",
      "qIAT  Faking Low           0.526297 6.888053e-04         0.53***           0.089989 0.591067            0.09\n",
      "qIAT Faking High           0.720422 2.328488e-07         0.72***           0.278463 0.086060            0.28\n",
      "qIAT     Overall           0.616432 8.445834e-14         0.62***           0.428661 0.000001         0.43***\n"
     ]
    }
   ],
   "source": [
    "# Helper function to compute correlation with significance\n",
    "def compute_correlation_with_test(x, y):\n",
    "    \"\"\"Compute Pearson correlation and test significance.\"\"\"\n",
    "    # Remove missing values\n",
    "    pairs = pd.DataFrame({\n",
    "        'x': pd.to_numeric(x, errors='coerce'),\n",
    "        'y': pd.to_numeric(y, errors='coerce')\n",
    "    }).dropna()\n",
    "    \n",
    "    if len(pairs) < 2:\n",
    "        return np.nan, np.nan, \"N/A\"\n",
    "    \n",
    "    # Compute correlation\n",
    "    corr, p_value = pearsonr(pairs['x'], pairs['y'])\n",
    "    \n",
    "    # Format with significance\n",
    "    sig = add_significance(p_value)\n",
    "    formatted = f\"{corr:.2f}{sig}\"\n",
    "    \n",
    "    return corr, p_value, formatted\n",
    "\n",
    "# Group mapping: 0 = Control, 1 = Faking Low, 2 = Faking High\n",
    "group_names = {0: 'Control', 1: 'Faking Low', 2: 'Faking High'}\n",
    "\n",
    "# Initialize results dictionary\n",
    "# Structure: results[task][group][time] = (correlation, p_value, formatted_string)\n",
    "results = {\n",
    "    'IAT': {},\n",
    "    'qIAT': {}\n",
    "}\n",
    "\n",
    "# Process IAT\n",
    "iat_data = df[df['task'].str.upper() == 'IAT'].copy()\n",
    "\n",
    "for group_num, group_name in group_names.items():\n",
    "    group_iat = iat_data[iat_data['group'] == group_num].copy()\n",
    "    \n",
    "    # Time 1: IAT D-score with Questionnaire score\n",
    "    t1_dscore = pd.to_numeric(group_iat['t1_dscore'], errors='coerce')\n",
    "    t1_ques = pd.to_numeric(group_iat['t1_ques'], errors='coerce')\n",
    "    corr_t1, p_t1, fmt_t1 = compute_correlation_with_test(t1_dscore, t1_ques)\n",
    "    \n",
    "    # Time 2: IAT D-score with Questionnaire score\n",
    "    t2_dscore = pd.to_numeric(group_iat['t2_dscore'], errors='coerce')\n",
    "    t2_ques = pd.to_numeric(group_iat['t2_ques'], errors='coerce')\n",
    "    corr_t2, p_t2, fmt_t2 = compute_correlation_with_test(t2_dscore, t2_ques)\n",
    "    \n",
    "    results['IAT'][group_name] = {\n",
    "        'Time1': (corr_t1, p_t1, fmt_t1),\n",
    "        'Time2': (corr_t2, p_t2, fmt_t2)\n",
    "    }\n",
    "\n",
    "# IAT Overall\n",
    "t1_dscore_all = pd.to_numeric(iat_data['t1_dscore'], errors='coerce')\n",
    "t1_ques_all = pd.to_numeric(iat_data['t1_ques'], errors='coerce')\n",
    "corr_t1_all, p_t1_all, fmt_t1_all = compute_correlation_with_test(t1_dscore_all, t1_ques_all)\n",
    "\n",
    "t2_dscore_all = pd.to_numeric(iat_data['t2_dscore'], errors='coerce')\n",
    "t2_ques_all = pd.to_numeric(iat_data['t2_ques'], errors='coerce')\n",
    "corr_t2_all, p_t2_all, fmt_t2_all = compute_correlation_with_test(t2_dscore_all, t2_ques_all)\n",
    "\n",
    "results['IAT']['Overall'] = {\n",
    "    'Time1': (corr_t1_all, p_t1_all, fmt_t1_all),\n",
    "    'Time2': (corr_t2_all, p_t2_all, fmt_t2_all)\n",
    "}\n",
    "\n",
    "# Process qIAT\n",
    "qiat_data = df[df['task'].str.upper() == 'QIAT'].copy()\n",
    "\n",
    "for group_num, group_name in group_names.items():\n",
    "    group_qiat = qiat_data[qiat_data['group'] == group_num].copy()\n",
    "    \n",
    "    # Time 1: qIAT D-score with Questionnaire score\n",
    "    t1_dscore = pd.to_numeric(group_qiat['t1_dscore'], errors='coerce')\n",
    "    t1_ques = pd.to_numeric(group_qiat['t1_ques'], errors='coerce')\n",
    "    corr_t1, p_t1, fmt_t1 = compute_correlation_with_test(t1_dscore, t1_ques)\n",
    "    \n",
    "    # Time 2: qIAT D-score with Questionnaire score\n",
    "    t2_dscore = pd.to_numeric(group_qiat['t2_dscore'], errors='coerce')\n",
    "    t2_ques = pd.to_numeric(group_qiat['t2_ques'], errors='coerce')\n",
    "    corr_t2, p_t2, fmt_t2 = compute_correlation_with_test(t2_dscore, t2_ques)\n",
    "    \n",
    "    results['qIAT'][group_name] = {\n",
    "        'Time1': (corr_t1, p_t1, fmt_t1),\n",
    "        'Time2': (corr_t2, p_t2, fmt_t2)\n",
    "    }\n",
    "\n",
    "# qIAT Overall\n",
    "t1_dscore_all = pd.to_numeric(qiat_data['t1_dscore'], errors='coerce')\n",
    "t1_ques_all = pd.to_numeric(qiat_data['t1_ques'], errors='coerce')\n",
    "corr_t1_all, p_t1_all, fmt_t1_all = compute_correlation_with_test(t1_dscore_all, t1_ques_all)\n",
    "\n",
    "t2_dscore_all = pd.to_numeric(qiat_data['t2_dscore'], errors='coerce')\n",
    "t2_ques_all = pd.to_numeric(qiat_data['t2_ques'], errors='coerce')\n",
    "corr_t2_all, p_t2_all, fmt_t2_all = compute_correlation_with_test(t2_dscore_all, t2_ques_all)\n",
    "\n",
    "results['qIAT']['Overall'] = {\n",
    "    'Time1': (corr_t1_all, p_t1_all, fmt_t1_all),\n",
    "    'Time2': (corr_t2_all, p_t2_all, fmt_t2_all)\n",
    "}\n",
    "\n",
    "# Create formatted table\n",
    "print(\"Table 5. Correlations between implicit tasks and the explicit measure in Study 1\")\n",
    "print(\"=\" * 120)\n",
    "header1 = f\"{'Measure':<10} {'Control':<30} {'Faking Low':<30} {'Faking High':<30} {'Overall':<30}\"\n",
    "print(header1)\n",
    "header2 = f\"{'':<10} {'Time1':<14} {'Time2':<14} {'Time1':<14} {'Time2':<14} {'Time1':<14} {'Time2':<14} {'Time1':<14} {'Time2':<14}\"\n",
    "print(header2)\n",
    "print(\"-\" * 120)\n",
    "\n",
    "for task in ['IAT', 'qIAT']:\n",
    "    row = f\"{task:<10}\"\n",
    "    \n",
    "    for group_name in ['Control', 'Faking Low', 'Faking High', 'Overall']:\n",
    "        if group_name in results[task]:\n",
    "            fmt_t1 = results[task][group_name]['Time1'][2]\n",
    "            fmt_t2 = results[task][group_name]['Time2'][2]\n",
    "            row += f\"{fmt_t1:<14} {fmt_t2:<14}\"\n",
    "        else:\n",
    "            row += f\"{'N/A':<14} {'N/A':<14}\"\n",
    "    \n",
    "    print(row)\n",
    "\n",
    "print(\"=\" * 120)\n",
    "print(\"Note: * p < .05, ** p < .01, *** p < .001\")\n",
    "\n",
    "# Create detailed DataFrame with all information\n",
    "table_data = []\n",
    "for task in ['IAT', 'qIAT']:\n",
    "    for group_name in ['Control', 'Faking Low', 'Faking High', 'Overall']:\n",
    "        if group_name in results[task]:\n",
    "            corr_t1, p_t1, fmt_t1 = results[task][group_name]['Time1']\n",
    "            corr_t2, p_t2, fmt_t2 = results[task][group_name]['Time2']\n",
    "            table_data.append({\n",
    "                'Task': task,\n",
    "                'Group': group_name,\n",
    "                'Time1_Correlation': corr_t1,\n",
    "                'Time1_p': p_t1,\n",
    "                'Time1_Formatted': fmt_t1,\n",
    "                'Time2_Correlation': corr_t2,\n",
    "                'Time2_p': p_t2,\n",
    "                'Time2_Formatted': fmt_t2\n",
    "            })\n",
    "\n",
    "table_df = pd.DataFrame(table_data)\n",
    "print(\"\\n\\nDetailed table (DataFrame format):\")\n",
    "print(table_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a07741c",
   "metadata": {},
   "source": [
    "## Implicit-Explicit Convergent Validity Comparison (Time1)\n",
    "\n",
    "Compare qIAT and IAT correlations with explicit measure at Time1 using Fisher z test to determine if qIAT shows stronger implicit-explicit relationship than IAT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5a055768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qIAT Time1 correlation with explicit measure:\n",
      "  r = 0.62, p = 0.0000, n = 119\n",
      "\n",
      "IAT Time1 correlation with explicit measure:\n",
      "  r = 0.39, p = 0.0000, n = 140\n",
      "\n",
      "============================================================\n",
      "Fisher z test comparing qIAT vs IAT correlations:\n",
      "============================================================\n",
      "qIAT correlation: r = 0.62, n = 119\n",
      "IAT correlation: r = 0.39, n = 140\n",
      "\n",
      "Fisher z statistic: z = 2.43\n",
      "p-value: p = 0.0153\n",
      "\n",
      "Conclusion: The difference is significant (p = 0.0153)\n",
      "qIAT exhibits a stronger implicit-explicit relationship than IAT.\n",
      "\n",
      "============================================================\n",
      "SUMMARY (for article):\n",
      "============================================================\n",
      "qIAT-explicit correlation (Time1): r = 0.62, p < .001\n",
      "IAT-explicit correlation (Time1): r = 0.39, p < .001\n",
      "Fisher z test: z = 2.43, p = 0.0153\n"
     ]
    }
   ],
   "source": [
    "# Get overall Time1 correlations for qIAT and IAT\n",
    "# These are the correlations from the \"Overall\" group in Table 5\n",
    "\n",
    "# qIAT Time1 correlation with explicit measure\n",
    "qiat_t1_data = df[df['task'].str.upper() == 'QIAT'].copy()\n",
    "qiat_t1_dscore = pd.to_numeric(qiat_t1_data['t1_dscore'], errors='coerce')\n",
    "qiat_t1_ques = pd.to_numeric(qiat_t1_data['t1_ques'], errors='coerce')\n",
    "\n",
    "qiat_t1_pairs = pd.DataFrame({\n",
    "    'dscore': qiat_t1_dscore,\n",
    "    'ques': qiat_t1_ques\n",
    "}).dropna()\n",
    "\n",
    "qiat_t1_corr, qiat_t1_p = pearsonr(qiat_t1_pairs['dscore'], qiat_t1_pairs['ques'])\n",
    "qiat_t1_n = len(qiat_t1_pairs)\n",
    "\n",
    "print(f\"qIAT Time1 correlation with explicit measure:\")\n",
    "print(f\"  r = {qiat_t1_corr:.2f}, p = {qiat_t1_p:.4f}, n = {qiat_t1_n}\")\n",
    "\n",
    "# IAT Time1 correlation with explicit measure\n",
    "iat_t1_data = df[df['task'].str.upper() == 'IAT'].copy()\n",
    "iat_t1_dscore = pd.to_numeric(iat_t1_data['t1_dscore'], errors='coerce')\n",
    "iat_t1_ques = pd.to_numeric(iat_t1_data['t1_ques'], errors='coerce')\n",
    "\n",
    "iat_t1_pairs = pd.DataFrame({\n",
    "    'dscore': iat_t1_dscore,\n",
    "    'ques': iat_t1_ques\n",
    "}).dropna()\n",
    "\n",
    "iat_t1_corr, iat_t1_p = pearsonr(iat_t1_pairs['dscore'], iat_t1_pairs['ques'])\n",
    "iat_t1_n = len(iat_t1_pairs)\n",
    "\n",
    "print(f\"\\nIAT Time1 correlation with explicit measure:\")\n",
    "print(f\"  r = {iat_t1_corr:.2f}, p = {iat_t1_p:.4f}, n = {iat_t1_n}\")\n",
    "\n",
    "# Fisher z test to compare the two correlations\n",
    "# Note: These are independent correlations (different participants in qIAT vs IAT groups)\n",
    "z_stat, p_value = compare_correlations(qiat_t1_corr, iat_t1_corr, qiat_t1_n, iat_t1_n)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Fisher z test comparing qIAT vs IAT correlations:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"qIAT correlation: r = {qiat_t1_corr:.2f}, n = {qiat_t1_n}\")\n",
    "print(f\"IAT correlation: r = {iat_t1_corr:.2f}, n = {iat_t1_n}\")\n",
    "print(f\"\\nFisher z statistic: z = {z_stat:.2f}\")\n",
    "print(f\"p-value: p = {p_value:.4f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(f\"\\nConclusion: The difference is significant (p = {p_value:.4f})\")\n",
    "    if qiat_t1_corr > iat_t1_corr:\n",
    "        print(\"qIAT exhibits a stronger implicit-explicit relationship than IAT.\")\n",
    "    else:\n",
    "        print(\"IAT exhibits a stronger implicit-explicit relationship than qIAT.\")\n",
    "else:\n",
    "    print(f\"\\nConclusion: The difference is not significant (p = {p_value:.4f})\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"SUMMARY (for article):\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"qIAT-explicit correlation (Time1): r = {qiat_t1_corr:.2f}, p < .001\")\n",
    "print(f\"IAT-explicit correlation (Time1): r = {iat_t1_corr:.2f}, p < .001\")\n",
    "print(f\"Fisher z test: z = {z_stat:.2f}, p = {p_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d050285e",
   "metadata": {},
   "source": [
    "## Fakeability Analysis: Self-Report (Questionnaire)\n",
    "\n",
    "Conduct mixed ANOVA for self-report measure with Time (within-subject) and Group (between-subject) factors, followed by planned contrasts (paired t-tests) for Faking High and Faking Low groups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "15b33fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Participants with complete questionnaire data: 259\n",
      "Group distribution:\n",
      "group\n",
      "0    89\n",
      "1    85\n",
      "2    85\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Data prepared for ANOVA: 518 observations\n",
      "\n",
      "============================================================\n",
      "Mixed ANOVA Results for Self-Report (Questionnaire)\n",
      "============================================================\n",
      "        Source            SS  DF1  DF2            MS           F  \\\n",
      "0        group  31130.507240    2  256  15565.253620  175.575779   \n",
      "1         time    490.378378    1  256    490.378378   15.898589   \n",
      "2  Interaction  23802.520498    2  256  11901.260249  385.851520   \n",
      "\n",
      "          p-unc       np2  eps  \n",
      "0  9.832977e-49  0.578359  NaN  \n",
      "1  8.724211e-05  0.058472  1.0  \n",
      "2  5.440733e-78  0.750901  NaN  \n",
      "\n",
      "------------------------------------------------------------\n",
      "ANOVA Summary:\n",
      "------------------------------------------------------------\n",
      "Group: F(?, 256) = 175.58, p = 0.0000, ηp² = 0.58\n",
      "Time: F(?, 256) = 15.90, p = 0.0001, ηp² = 0.06\n",
      "Interaction: F(?, 256) = 385.85, p = 0.0000, ηp² = 0.75\n",
      "\n",
      "============================================================\n",
      "Descriptive Statistics for Planned Contrasts\n",
      "============================================================\n",
      "\n",
      "Faking Low group (n = 85):\n",
      "  Time1: M = 26.61, SD = 8.16\n",
      "  Time2: M = 12.54, SD = 5.14\n",
      "\n",
      "Faking High group (n = 85):\n",
      "  Time1: M = 29.04, SD = 9.20\n",
      "  Time2: M = 48.35, SD = 4.28\n",
      "\n",
      "============================================================\n",
      "Planned Contrasts (Paired t-tests)\n",
      "============================================================\n",
      "\n",
      "Faking Low group:\n",
      "  Paired t-test: t(84) = 13.79, p = 0.0000\n",
      "  Cohen's d = 1.50\n",
      "  Mean difference = 14.07, SD of differences = 9.41\n",
      "\n",
      "Faking High group:\n",
      "  Paired t-test: t(84) = 18.47, p = 0.0000\n",
      "  Cohen's d = 2.00\n",
      "  Mean difference = -19.32, SD of differences = 9.64\n",
      "\n",
      "============================================================\n",
      "SUMMARY (for article):\n",
      "============================================================\n",
      "Mixed ANOVA for Self-Report:\n",
      "  (Note: Full ANOVA results require specialized output)\n",
      "\n",
      "Planned Contrasts:\n",
      "  Faking Low: t(84) = 13.79, p < .001, d = 1.50\n",
      "  Faking High: t(84) = -18.47, p < .001, d = -2.00\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for mixed ANOVA\n",
    "# Need long format: participant, group, time, score\n",
    "ques_data = df[['id', 'group', 't1_ques', 't2_ques']].copy()\n",
    "\n",
    "# Convert to numeric\n",
    "ques_data['group'] = pd.to_numeric(ques_data['group'], errors='coerce')\n",
    "ques_data['t1_ques'] = pd.to_numeric(ques_data['t1_ques'], errors='coerce')\n",
    "ques_data['t2_ques'] = pd.to_numeric(ques_data['t2_ques'], errors='coerce')\n",
    "\n",
    "# Remove rows with missing data\n",
    "ques_data = ques_data.dropna()\n",
    "\n",
    "print(f\"Participants with complete questionnaire data: {len(ques_data)}\")\n",
    "print(f\"Group distribution:\")\n",
    "print(ques_data['group'].value_counts().sort_index())\n",
    "\n",
    "# Reshape to long format for ANOVA\n",
    "ques_long = pd.melt(\n",
    "    ques_data,\n",
    "    id_vars=['id', 'group'],\n",
    "    value_vars=['t1_ques', 't2_ques'],\n",
    "    var_name='time',\n",
    "    value_name='score'\n",
    ")\n",
    "\n",
    "# Recode time: t1_ques -> 1, t2_ques -> 2\n",
    "ques_long['time'] = ques_long['time'].map({'t1_ques': 1, 't2_ques': 2})\n",
    "\n",
    "# Recode group labels for clarity (0=Control, 1=Faking Low, 2=Faking High)\n",
    "ques_long['group_label'] = ques_long['group'].map({0: 'Control', 1: 'Faking Low', 2: 'Faking High'})\n",
    "\n",
    "print(f\"\\nData prepared for ANOVA: {len(ques_long)} observations\")\n",
    "\n",
    "# Mixed ANOVA using pingouin\n",
    "# Run mixed ANOVA using pingouin\n",
    "aov = pg.mixed_anova(\n",
    "    data=ques_long,\n",
    "    dv='score',\n",
    "    within='time',\n",
    "    between='group',\n",
    "    subject='id'\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Mixed ANOVA Results for Self-Report (Questionnaire)\")\n",
    "print(\"=\"*60)\n",
    "print(aov)\n",
    "\n",
    "# Extract and format results\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"ANOVA Summary:\")\n",
    "print(\"-\"*60)\n",
    "for idx, row in aov.iterrows():\n",
    "    effect = row['Source']\n",
    "    if 'time' in effect.lower() and 'group' in effect.lower() and '*' in effect:\n",
    "        effect_name = \"Group × Time\"\n",
    "    elif 'time' in effect.lower():\n",
    "        effect_name = \"Time\"\n",
    "    elif 'group' in effect.lower():\n",
    "        effect_name = \"Group\"\n",
    "    else:\n",
    "        effect_name = effect\n",
    "    \n",
    "    f_val = row['F']\n",
    "    p_val = row['p-unc']\n",
    "    eta_sq = row.get('np2', np.nan)  # partial eta squared\n",
    "    \n",
    "    # Get degrees of freedom\n",
    "    df1 = row.get('DF', '?')\n",
    "    df2 = row.get('DF2', '?')\n",
    "    \n",
    "    print(f\"{effect_name}: F({df1}, {df2}) = {f_val:.2f}, p = {p_val:.4f}, ηp² = {eta_sq:.2f}\")\n",
    "\n",
    "# Compute descriptive statistics for planned contrasts\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Descriptive Statistics for Planned Contrasts\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Faking Low group (group == 1)\n",
    "faking_low = ques_data[ques_data['group'] == 1].copy()\n",
    "fl_t1_mean = faking_low['t1_ques'].mean()\n",
    "fl_t1_sd = faking_low['t1_ques'].std(ddof=1)\n",
    "fl_t2_mean = faking_low['t2_ques'].mean()\n",
    "fl_t2_sd = faking_low['t2_ques'].std(ddof=1)\n",
    "fl_n = len(faking_low)\n",
    "\n",
    "print(f\"\\nFaking Low group (n = {fl_n}):\")\n",
    "print(f\"  Time1: M = {fl_t1_mean:.2f}, SD = {fl_t1_sd:.2f}\")\n",
    "print(f\"  Time2: M = {fl_t2_mean:.2f}, SD = {fl_t2_sd:.2f}\")\n",
    "\n",
    "# Faking High group (group == 2)\n",
    "faking_high = ques_data[ques_data['group'] == 2].copy()\n",
    "fh_t1_mean = faking_high['t1_ques'].mean()\n",
    "fh_t1_sd = faking_high['t1_ques'].std(ddof=1)\n",
    "fh_t2_mean = faking_high['t2_ques'].mean()\n",
    "fh_t2_sd = faking_high['t2_ques'].std(ddof=1)\n",
    "fh_n = len(faking_high)\n",
    "\n",
    "print(f\"\\nFaking High group (n = {fh_n}):\")\n",
    "print(f\"  Time1: M = {fh_t1_mean:.2f}, SD = {fh_t1_sd:.2f}\")\n",
    "print(f\"  Time2: M = {fh_t2_mean:.2f}, SD = {fh_t2_sd:.2f}\")\n",
    "\n",
    "# Planned contrasts: Paired t-tests\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Planned Contrasts (Paired t-tests)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Faking Low: Time1 vs Time2\n",
    "fl_t1 = faking_low['t1_ques'].values\n",
    "fl_t2 = faking_low['t2_ques'].values\n",
    "fl_t_stat, fl_t_p = ttest_rel(fl_t1, fl_t2)\n",
    "\n",
    "# Cohen's d for paired samples: mean difference / SD of differences\n",
    "fl_diff = fl_t1 - fl_t2\n",
    "fl_cohens_d = fl_diff.mean() / fl_diff.std(ddof=1)\n",
    "\n",
    "print(f\"\\nFaking Low group:\")\n",
    "print(f\"  Paired t-test: t({fl_n-1}) = {fl_t_stat:.2f}, p = {fl_t_p:.4f}\")\n",
    "print(f\"  Cohen's d = {fl_cohens_d:.2f}\")\n",
    "print(f\"  Mean difference = {fl_diff.mean():.2f}, SD of differences = {fl_diff.std(ddof=1):.2f}\")\n",
    "\n",
    "# Faking High: Time1 vs Time2\n",
    "fh_t1 = faking_high['t1_ques'].values\n",
    "fh_t2 = faking_high['t2_ques'].values\n",
    "fh_t_stat, fh_t_p = ttest_rel(fh_t1, fh_t2)\n",
    "\n",
    "# Cohen's d for paired samples: mean difference / SD of differences\n",
    "fh_diff = fh_t1 - fh_t2\n",
    "fh_cohens_d = fh_diff.mean() / fh_diff.std(ddof=1)\n",
    "\n",
    "print(f\"\\nFaking High group:\")\n",
    "print(f\"  Paired t-test: t({fh_n-1}) = {abs(fh_t_stat):.2f}, p = {fh_t_p:.4f}\")\n",
    "print(f\"  Cohen's d = {abs(fh_cohens_d):.2f}\")\n",
    "print(f\"  Mean difference = {fh_diff.mean():.2f}, SD of differences = {fh_diff.std(ddof=1):.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY (for article):\")\n",
    "print(\"=\"*60)\n",
    "print(\"Mixed ANOVA for Self-Report:\")\n",
    "print(\"  (Note: Full ANOVA results require specialized output)\")\n",
    "print(\"\\nPlanned Contrasts:\")\n",
    "print(f\"  Faking Low: t({fl_n-1}) = {fl_t_stat:.2f}, p < .001, d = {fl_cohens_d:.2f}\")\n",
    "print(f\"  Faking High: t({fh_n-1}) = {fh_t_stat:.2f}, p < .001, d = {fh_cohens_d:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493b0cdb",
   "metadata": {},
   "source": [
    "## Fakeability Analysis: IAT\n",
    "\n",
    "Conduct mixed ANOVA for IAT measure with Time (within-subject) and Group (between-subject) factors, followed by planned contrasts (paired t-tests) for Faking High and Faking Low groups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3a3e190e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IAT participants with complete data: 140\n",
      "Group distribution:\n",
      "group\n",
      "0    47\n",
      "1    47\n",
      "2    46\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Data prepared for ANOVA: 280 observations\n",
      "\n",
      "============================================================\n",
      "Mixed ANOVA Results for IAT\n",
      "============================================================\n",
      "        Source        SS  DF1  DF2        MS          F     p-unc       np2  \\\n",
      "0        group  0.130281    2  137  0.065140   0.150065  0.860793  0.002186   \n",
      "1         time  1.404793    1  137  1.404793  11.552529  0.000886  0.077767   \n",
      "2  Interaction  0.347161    2  137  0.173580   1.427466  0.243462  0.020414   \n",
      "\n",
      "   eps  \n",
      "0  NaN  \n",
      "1  1.0  \n",
      "2  NaN  \n",
      "\n",
      "------------------------------------------------------------\n",
      "ANOVA Summary:\n",
      "------------------------------------------------------------\n",
      "Group: F(2, 137) = 0.15, p = 0.8608, ηp² = 0.00\n",
      "Time: F(1, 137) = 11.55, p = 0.0009, ηp² = 0.08\n",
      "Interaction: F(2, 137) = 1.43, p = 0.2435, ηp² = 0.02\n",
      "\n",
      "============================================================\n",
      "Descriptive Statistics for Planned Contrasts\n",
      "============================================================\n",
      "\n",
      "Faking Low group (n = 47):\n",
      "  Time1: M = -0.41, SD = 0.61\n",
      "  Time2: M = -0.35, SD = 0.44\n",
      "\n",
      "Faking High group (n = 46):\n",
      "  Time1: M = -0.47, SD = 0.56\n",
      "  Time2: M = -0.25, SD = 0.53\n",
      "\n",
      "============================================================\n",
      "Planned Contrasts (Paired t-tests)\n",
      "============================================================\n",
      "\n",
      "Faking Low group:\n",
      "  Paired t-test: t(46) = -0.71, p = 0.4801\n",
      "  Cohen's d = -0.10\n",
      "  Mean difference = -0.05, SD of differences = 0.51\n",
      "\n",
      "Faking High group:\n",
      "  Paired t-test: t(45) = 3.35, p = 0.0016\n",
      "  Cohen's d = 0.49\n",
      "  Mean difference = -0.23, SD of differences = 0.46\n",
      "\n",
      "============================================================\n",
      "SUMMARY (for article):\n",
      "============================================================\n",
      "Mixed ANOVA for IAT:\n",
      "  (See ANOVA Summary above for F-statistics, p-values, and ηp²)\n",
      "\n",
      "Planned Contrasts:\n",
      "  Faking Low: t(46) = -0.71, p = 0.4801, d = -0.10\n",
      "  Faking High: t(45) = 3.35, p = 0.0016, d = 0.49\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for mixed ANOVA - IAT only\n",
    "# Filter to IAT task\n",
    "iat_data = df[df['task'].str.upper() == 'IAT'].copy()\n",
    "\n",
    "# Prepare data: participant, group, time, score\n",
    "iat_ques_data = iat_data[['id', 'group', 't1_dscore', 't2_dscore']].copy()\n",
    "\n",
    "# Convert to numeric\n",
    "iat_ques_data['group'] = pd.to_numeric(iat_ques_data['group'], errors='coerce')\n",
    "iat_ques_data['t1_dscore'] = pd.to_numeric(iat_ques_data['t1_dscore'], errors='coerce')\n",
    "iat_ques_data['t2_dscore'] = pd.to_numeric(iat_ques_data['t2_dscore'], errors='coerce')\n",
    "\n",
    "# Remove rows with missing data\n",
    "iat_ques_data = iat_ques_data.dropna()\n",
    "\n",
    "print(f\"IAT participants with complete data: {len(iat_ques_data)}\")\n",
    "print(f\"Group distribution:\")\n",
    "print(iat_ques_data['group'].value_counts().sort_index())\n",
    "\n",
    "# Reshape to long format for ANOVA\n",
    "iat_long = pd.melt(\n",
    "    iat_ques_data,\n",
    "    id_vars=['id', 'group'],\n",
    "    value_vars=['t1_dscore', 't2_dscore'],\n",
    "    var_name='time',\n",
    "    value_name='score'\n",
    ")\n",
    "\n",
    "# Recode time: t1_dscore -> 1, t2_dscore -> 2\n",
    "iat_long['time'] = iat_long['time'].map({'t1_dscore': 1, 't2_dscore': 2})\n",
    "\n",
    "print(f\"\\nData prepared for ANOVA: {len(iat_long)} observations\")\n",
    "\n",
    "# Mixed ANOVA using pingouin\n",
    "aov = pg.mixed_anova(\n",
    "    data=iat_long,\n",
    "    dv='score',\n",
    "    within='time',\n",
    "    between='group',\n",
    "    subject='id'\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Mixed ANOVA Results for IAT\")\n",
    "print(\"=\"*60)\n",
    "print(aov)\n",
    "\n",
    "# Extract and format results\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"ANOVA Summary:\")\n",
    "print(\"-\"*60)\n",
    "for idx, row in aov.iterrows():\n",
    "    effect = row['Source']\n",
    "    if 'time' in effect.lower() and 'group' in effect.lower() and '*' in effect:\n",
    "        effect_name = \"Group × Time\"\n",
    "    elif 'time' in effect.lower():\n",
    "        effect_name = \"Time\"\n",
    "    elif 'group' in effect.lower():\n",
    "        effect_name = \"Group\"\n",
    "    else:\n",
    "        effect_name = effect\n",
    "    \n",
    "    f_val = row['F']\n",
    "    p_val = row['p-unc']\n",
    "    eta_sq = row.get('np2', np.nan)  # partial eta squared\n",
    "    \n",
    "    # Get degrees of freedom\n",
    "    df1 = row.get('DF1', row.get('DF', '?'))\n",
    "    df2 = row.get('DF2', '?')\n",
    "    \n",
    "    print(f\"{effect_name}: F({df1}, {df2}) = {f_val:.2f}, p = {p_val:.4f}, ηp² = {eta_sq:.2f}\")\n",
    "\n",
    "# Compute descriptive statistics for planned contrasts\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Descriptive Statistics for Planned Contrasts\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Faking Low group (group == 1)\n",
    "iat_faking_low = iat_ques_data[iat_ques_data['group'] == 1].copy()\n",
    "iat_fl_t1_mean = iat_faking_low['t1_dscore'].mean()\n",
    "iat_fl_t1_sd = iat_faking_low['t1_dscore'].std(ddof=1)\n",
    "iat_fl_t2_mean = iat_faking_low['t2_dscore'].mean()\n",
    "iat_fl_t2_sd = iat_faking_low['t2_dscore'].std(ddof=1)\n",
    "iat_fl_n = len(iat_faking_low)\n",
    "\n",
    "print(f\"\\nFaking Low group (n = {iat_fl_n}):\")\n",
    "print(f\"  Time1: M = {iat_fl_t1_mean:.2f}, SD = {iat_fl_t1_sd:.2f}\")\n",
    "print(f\"  Time2: M = {iat_fl_t2_mean:.2f}, SD = {iat_fl_t2_sd:.2f}\")\n",
    "\n",
    "# Faking High group (group == 2)\n",
    "iat_faking_high = iat_ques_data[iat_ques_data['group'] == 2].copy()\n",
    "iat_fh_t1_mean = iat_faking_high['t1_dscore'].mean()\n",
    "iat_fh_t1_sd = iat_faking_high['t1_dscore'].std(ddof=1)\n",
    "iat_fh_t2_mean = iat_faking_high['t2_dscore'].mean()\n",
    "iat_fh_t2_sd = iat_faking_high['t2_dscore'].std(ddof=1)\n",
    "iat_fh_n = len(iat_faking_high)\n",
    "\n",
    "print(f\"\\nFaking High group (n = {iat_fh_n}):\")\n",
    "print(f\"  Time1: M = {iat_fh_t1_mean:.2f}, SD = {iat_fh_t1_sd:.2f}\")\n",
    "print(f\"  Time2: M = {iat_fh_t2_mean:.2f}, SD = {iat_fh_t2_sd:.2f}\")\n",
    "\n",
    "# Planned contrasts: Paired t-tests\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Planned Contrasts (Paired t-tests)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Faking Low: Time1 vs Time2\n",
    "iat_fl_t1 = iat_faking_low['t1_dscore'].values\n",
    "iat_fl_t2 = iat_faking_low['t2_dscore'].values\n",
    "iat_fl_t_stat, iat_fl_t_p = ttest_rel(iat_fl_t1, iat_fl_t2)\n",
    "\n",
    "# Cohen's d for paired samples: mean difference / SD of differences\n",
    "iat_fl_diff = iat_fl_t1 - iat_fl_t2\n",
    "iat_fl_cohens_d = iat_fl_diff.mean() / iat_fl_diff.std(ddof=1)\n",
    "\n",
    "print(f\"\\nFaking Low group:\")\n",
    "print(f\"  Paired t-test: t({iat_fl_n-1}) = {iat_fl_t_stat:.2f}, p = {iat_fl_t_p:.4f}\")\n",
    "print(f\"  Cohen's d = {iat_fl_cohens_d:.2f}\")\n",
    "print(f\"  Mean difference = {iat_fl_diff.mean():.2f}, SD of differences = {iat_fl_diff.std(ddof=1):.2f}\")\n",
    "\n",
    "# Faking High: Time1 vs Time2\n",
    "iat_fh_t1 = iat_faking_high['t1_dscore'].values\n",
    "iat_fh_t2 = iat_faking_high['t2_dscore'].values\n",
    "iat_fh_t_stat, iat_fh_t_p = ttest_rel(iat_fh_t1, iat_fh_t2)\n",
    "\n",
    "# Cohen's d for paired samples: mean difference / SD of differences\n",
    "iat_fh_diff = iat_fh_t1 - iat_fh_t2\n",
    "iat_fh_cohens_d = iat_fh_diff.mean() / iat_fh_diff.std(ddof=1)\n",
    "\n",
    "print(f\"\\nFaking High group:\")\n",
    "print(f\"  Paired t-test: t({iat_fh_n-1}) = {abs(iat_fh_t_stat):.2f}, p = {iat_fh_t_p:.4f}\")\n",
    "print(f\"  Cohen's d = {abs(iat_fh_cohens_d):.2f}\")\n",
    "print(f\"  Mean difference = {iat_fh_diff.mean():.2f}, SD of differences = {iat_fh_diff.std(ddof=1):.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY (for article):\")\n",
    "print(\"=\"*60)\n",
    "print(\"Mixed ANOVA for IAT:\")\n",
    "print(\"  (See ANOVA Summary above for F-statistics, p-values, and ηp²)\")\n",
    "print(\"\\nPlanned Contrasts:\")\n",
    "print(f\"  Faking Low: t({iat_fl_n-1}) = {iat_fl_t_stat:.2f}, p = {iat_fl_t_p:.4f}, d = {iat_fl_cohens_d:.2f}\")\n",
    "print(f\"  Faking High: t({iat_fh_n-1}) = {abs(iat_fh_t_stat):.2f}, p = {iat_fh_t_p:.4f}, d = {abs(iat_fh_cohens_d):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c67acb",
   "metadata": {},
   "source": [
    "## Fakeability Analysis: qIAT\n",
    "\n",
    "Conduct mixed ANOVA for qIAT measure with Time (within-subject) and Group (between-subject) factors, followed by planned contrasts (paired t-tests) for Faking High and Faking Low groups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "74695a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qIAT participants with complete data: 119\n",
      "Group distribution:\n",
      "group\n",
      "0    42\n",
      "1    38\n",
      "2    39\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Data prepared for ANOVA: 238 observations\n",
      "\n",
      "============================================================\n",
      "Mixed ANOVA Results for qIAT\n",
      "============================================================\n",
      "        Source        SS  DF1  DF2        MS         F     p-unc       np2  \\\n",
      "0        group  6.353222    2  116  3.176611  3.149529  0.046561  0.051505   \n",
      "1         time  0.311138    1  116  0.311138  1.516125  0.220697  0.012901   \n",
      "2  Interaction  0.583617    2  116  0.291808  1.421935  0.245420  0.023929   \n",
      "\n",
      "   eps  \n",
      "0  NaN  \n",
      "1  1.0  \n",
      "2  NaN  \n",
      "\n",
      "------------------------------------------------------------\n",
      "ANOVA Summary:\n",
      "------------------------------------------------------------\n",
      "Group: F(2, 116) = 3.15, p = 0.0466, ηp² = 0.05\n",
      "Time: F(1, 116) = 1.52, p = 0.2207, ηp² = 0.01\n",
      "Interaction: F(2, 116) = 1.42, p = 0.2454, ηp² = 0.02\n",
      "\n",
      "============================================================\n",
      "Descriptive Statistics for Planned Contrasts\n",
      "============================================================\n",
      "\n",
      "Faking Low group (n = 38):\n",
      "  Time1: M = -0.01, SD = 0.85\n",
      "  Time2: M = -0.05, SD = 0.64\n",
      "\n",
      "Faking High group (n = 39):\n",
      "  Time1: M = 0.21, SD = 1.02\n",
      "  Time2: M = 0.42, SD = 0.63\n",
      "\n",
      "============================================================\n",
      "Planned Contrasts (Paired t-tests)\n",
      "============================================================\n",
      "\n",
      "Faking Low group:\n",
      "  Paired t-test: t(37) = 0.36, p = 0.7244\n",
      "  Cohen's d = 0.06\n",
      "  Mean difference = 0.04, SD of differences = 0.62\n",
      "\n",
      "Faking High group:\n",
      "  Paired t-test: t(38) = 1.73, p = 0.0921\n",
      "  Cohen's d = 0.28\n",
      "  Mean difference = -0.21, SD of differences = 0.74\n",
      "\n",
      "============================================================\n",
      "SUMMARY (for article):\n",
      "============================================================\n",
      "Mixed ANOVA for qIAT:\n",
      "  (See ANOVA Summary above for F-statistics, p-values, and ηp²)\n",
      "\n",
      "Planned Contrasts:\n",
      "  Faking Low: t(37) = 0.36, p = 0.7244, d = 0.06\n",
      "  Faking High: t(38) = 1.73, p = 0.0921, d = 0.28\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for mixed ANOVA - qIAT only\n",
    "# Filter to qIAT task\n",
    "qiat_data = df[df['task'].str.upper() == 'QIAT'].copy()\n",
    "\n",
    "# Prepare data: participant, group, time, score\n",
    "qiat_ques_data = qiat_data[['id', 'group', 't1_dscore', 't2_dscore']].copy()\n",
    "\n",
    "# Convert to numeric\n",
    "qiat_ques_data['group'] = pd.to_numeric(qiat_ques_data['group'], errors='coerce')\n",
    "qiat_ques_data['t1_dscore'] = pd.to_numeric(qiat_ques_data['t1_dscore'], errors='coerce')\n",
    "qiat_ques_data['t2_dscore'] = pd.to_numeric(qiat_ques_data['t2_dscore'], errors='coerce')\n",
    "\n",
    "# Remove rows with missing data\n",
    "qiat_ques_data = qiat_ques_data.dropna()\n",
    "\n",
    "print(f\"qIAT participants with complete data: {len(qiat_ques_data)}\")\n",
    "print(f\"Group distribution:\")\n",
    "print(qiat_ques_data['group'].value_counts().sort_index())\n",
    "\n",
    "# Reshape to long format for ANOVA\n",
    "qiat_long = pd.melt(\n",
    "    qiat_ques_data,\n",
    "    id_vars=['id', 'group'],\n",
    "    value_vars=['t1_dscore', 't2_dscore'],\n",
    "    var_name='time',\n",
    "    value_name='score'\n",
    ")\n",
    "\n",
    "# Recode time: t1_dscore -> 1, t2_dscore -> 2\n",
    "qiat_long['time'] = qiat_long['time'].map({'t1_dscore': 1, 't2_dscore': 2})\n",
    "\n",
    "print(f\"\\nData prepared for ANOVA: {len(qiat_long)} observations\")\n",
    "\n",
    "# Mixed ANOVA using pingouin\n",
    "aov = pg.mixed_anova(\n",
    "    data=qiat_long,\n",
    "    dv='score',\n",
    "    within='time',\n",
    "    between='group',\n",
    "    subject='id'\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Mixed ANOVA Results for qIAT\")\n",
    "print(\"=\"*60)\n",
    "print(aov)\n",
    "\n",
    "# Extract and format results\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"ANOVA Summary:\")\n",
    "print(\"-\"*60)\n",
    "for idx, row in aov.iterrows():\n",
    "    effect = row['Source']\n",
    "    if 'time' in effect.lower() and 'group' in effect.lower() and '*' in effect:\n",
    "        effect_name = \"Group × Time\"\n",
    "    elif 'time' in effect.lower():\n",
    "        effect_name = \"Time\"\n",
    "    elif 'group' in effect.lower():\n",
    "        effect_name = \"Group\"\n",
    "    else:\n",
    "        effect_name = effect\n",
    "    \n",
    "    f_val = row['F']\n",
    "    p_val = row['p-unc']\n",
    "    eta_sq = row.get('np2', np.nan)  # partial eta squared\n",
    "    \n",
    "    # Get degrees of freedom\n",
    "    df1 = row.get('DF1', row.get('DF', '?'))\n",
    "    df2 = row.get('DF2', '?')\n",
    "    \n",
    "    print(f\"{effect_name}: F({df1}, {df2}) = {f_val:.2f}, p = {p_val:.4f}, ηp² = {eta_sq:.2f}\")\n",
    "\n",
    "# Compute descriptive statistics for planned contrasts\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Descriptive Statistics for Planned Contrasts\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Faking Low group (group == 1)\n",
    "qiat_faking_low = qiat_ques_data[qiat_ques_data['group'] == 1].copy()\n",
    "qiat_fl_t1_mean = qiat_faking_low['t1_dscore'].mean()\n",
    "qiat_fl_t1_sd = qiat_faking_low['t1_dscore'].std(ddof=1)\n",
    "qiat_fl_t2_mean = qiat_faking_low['t2_dscore'].mean()\n",
    "qiat_fl_t2_sd = qiat_faking_low['t2_dscore'].std(ddof=1)\n",
    "qiat_fl_n = len(qiat_faking_low)\n",
    "\n",
    "print(f\"\\nFaking Low group (n = {qiat_fl_n}):\")\n",
    "print(f\"  Time1: M = {qiat_fl_t1_mean:.2f}, SD = {qiat_fl_t1_sd:.2f}\")\n",
    "print(f\"  Time2: M = {qiat_fl_t2_mean:.2f}, SD = {qiat_fl_t2_sd:.2f}\")\n",
    "\n",
    "# Faking High group (group == 2)\n",
    "qiat_faking_high = qiat_ques_data[qiat_ques_data['group'] == 2].copy()\n",
    "qiat_fh_t1_mean = qiat_faking_high['t1_dscore'].mean()\n",
    "qiat_fh_t1_sd = qiat_faking_high['t1_dscore'].std(ddof=1)\n",
    "qiat_fh_t2_mean = qiat_faking_high['t2_dscore'].mean()\n",
    "qiat_fh_t2_sd = qiat_faking_high['t2_dscore'].std(ddof=1)\n",
    "qiat_fh_n = len(qiat_faking_high)\n",
    "\n",
    "print(f\"\\nFaking High group (n = {qiat_fh_n}):\")\n",
    "print(f\"  Time1: M = {qiat_fh_t1_mean:.2f}, SD = {qiat_fh_t1_sd:.2f}\")\n",
    "print(f\"  Time2: M = {qiat_fh_t2_mean:.2f}, SD = {qiat_fh_t2_sd:.2f}\")\n",
    "\n",
    "# Planned contrasts: Paired t-tests\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Planned Contrasts (Paired t-tests)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Faking Low: Time1 vs Time2\n",
    "qiat_fl_t1 = qiat_faking_low['t1_dscore'].values\n",
    "qiat_fl_t2 = qiat_faking_low['t2_dscore'].values\n",
    "qiat_fl_t_stat, qiat_fl_t_p = ttest_rel(qiat_fl_t1, qiat_fl_t2)\n",
    "\n",
    "# Cohen's d for paired samples: mean difference / SD of differences\n",
    "qiat_fl_diff = qiat_fl_t1 - qiat_fl_t2\n",
    "qiat_fl_cohens_d = qiat_fl_diff.mean() / qiat_fl_diff.std(ddof=1)\n",
    "\n",
    "print(f\"\\nFaking Low group:\")\n",
    "print(f\"  Paired t-test: t({qiat_fl_n-1}) = {qiat_fl_t_stat:.2f}, p = {qiat_fl_t_p:.4f}\")\n",
    "print(f\"  Cohen's d = {qiat_fl_cohens_d:.2f}\")\n",
    "print(f\"  Mean difference = {qiat_fl_diff.mean():.2f}, SD of differences = {qiat_fl_diff.std(ddof=1):.2f}\")\n",
    "\n",
    "# Faking High: Time1 vs Time2\n",
    "qiat_fh_t1 = qiat_faking_high['t1_dscore'].values\n",
    "qiat_fh_t2 = qiat_faking_high['t2_dscore'].values\n",
    "qiat_fh_t_stat, qiat_fh_t_p = ttest_rel(qiat_fh_t1, qiat_fh_t2)\n",
    "\n",
    "# Cohen's d for paired samples: mean difference / SD of differences\n",
    "qiat_fh_diff = qiat_fh_t1 - qiat_fh_t2\n",
    "qiat_fh_cohens_d = qiat_fh_diff.mean() / qiat_fh_diff.std(ddof=1)\n",
    "\n",
    "print(f\"\\nFaking High group:\")\n",
    "print(f\"  Paired t-test: t({qiat_fh_n-1}) = {abs(qiat_fh_t_stat):.2f}, p = {qiat_fh_t_p:.4f}\")\n",
    "print(f\"  Cohen's d = {abs(qiat_fh_cohens_d):.2f}\")\n",
    "print(f\"  Mean difference = {qiat_fh_diff.mean():.2f}, SD of differences = {qiat_fh_diff.std(ddof=1):.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY (for article):\")\n",
    "print(\"=\"*60)\n",
    "print(\"Mixed ANOVA for qIAT:\")\n",
    "print(\"  (See ANOVA Summary above for F-statistics, p-values, and ηp²)\")\n",
    "print(\"\\nPlanned Contrasts:\")\n",
    "print(f\"  Faking Low: t({qiat_fl_n-1}) = {qiat_fl_t_stat:.2f}, p = {qiat_fl_t_p:.4f}, d = {qiat_fl_cohens_d:.2f}\")\n",
    "print(f\"  Faking High: t({qiat_fh_n-1}) = {abs(qiat_fh_t_stat):.2f}, p = {qiat_fh_t_p:.4f}, d = {abs(qiat_fh_cohens_d):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae54a5a2",
   "metadata": {},
   "source": [
    "## Compute Normalized D-Scores (t1_com_d_norm and t2_com_d_norm)\n",
    "\n",
    "Compute normalized D-scores using Time1 baseline statistics for standardization. This enables comparison of faking effects across different measures (self-report, IAT, qIAT) by standardizing scores based on Time1 baseline statistics for each task separately.\n",
    "\n",
    "**Note:** This code modifies the combined participants file. Execute only when ready to update the data file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed7910e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../data/combined_participants_S1&S2.csv...\n",
      "Loaded 898 participants\n",
      "Original columns: 52\n",
      "\n",
      "t1_com_d_norm and t2_com_d_norm columns already exist. Skipping computation.\n",
      "\n",
      "Final columns: 52\n",
      "\n",
      "Saving to ../data/combined_participants_S1&S2.csv...\n"
     ]
    }
   ],
   "source": [
    "# Compute normalized D-scores using Time1 baseline statistics\n",
    "# This code removes t1_d_norm and t2_d_norm, and computes t1_com_d_norm and t2_com_d_norm\n",
    "\n",
    "\n",
    "# Define paths\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "combined_file = DATA_DIR / \"combined_participants_S1&S2.csv\"\n",
    "\n",
    "print(f\"Loading {combined_file}...\")\n",
    "# Load the CSV file (single header row)\n",
    "df_all_studies = pd.read_csv(combined_file)\n",
    "# CSV has single header row, so data starts from row 1\n",
    "# Reset index\n",
    "df_all_studies = df_all_studies.reset_index(drop=True)\n",
    "\n",
    "print(f\"Loaded {len(df_all_studies)} participants\")\n",
    "print(f\"Original columns: {len(df_all_studies.columns)}\")\n",
    "\n",
    "# Check if t1_com_d_norm and t2_com_d_norm already exist\n",
    "if 't1_com_d_norm' in df_all_studies.columns and 't2_com_d_norm' in df_all_studies.columns:\n",
    "    print(\"\\nt1_com_d_norm and t2_com_d_norm columns already exist. Skipping computation.\")\n",
    "else:\n",
    "    print(\"\\nComputing t1_com_d_norm and t2_com_d_norm...\")\n",
    "    \n",
    "    # Remove old columns if they exist\n",
    "    columns_to_remove = ['t1_d_norm', 't2_d_norm']\n",
    "    for col in columns_to_remove:\n",
    "        if col in df_all_studies.columns:\n",
    "            df_all_studies = df_all_studies.drop(columns=[col])\n",
    "            print(f\"Removed column: {col}\")\n",
    "    \n",
    "    # Convert necessary columns to appropriate types\n",
    "    df_all_studies['study'] = df_all_studies['study'].astype(str)\n",
    "    df_all_studies['task'] = df_all_studies['task'].astype(str)\n",
    "    df_all_studies['t1_dscore'] = pd.to_numeric(df_all_studies['t1_dscore'], errors='coerce')\n",
    "    df_all_studies['t2_dscore'] = pd.to_numeric(df_all_studies['t2_dscore'], errors='coerce')\n",
    "    \n",
    "    # Initialize new columns\n",
    "    df_all_studies['t1_com_d_norm'] = np.nan\n",
    "    df_all_studies['t2_com_d_norm'] = np.nan\n",
    "    \n",
    "    # Process each study and task separately\n",
    "    # Get unique study values from the data\n",
    "    unique_studies = df_all_studies['study'].astype(str).str.upper().unique()\n",
    "    print(f\"Found studies: {unique_studies}\")\n",
    "    \n",
    "    for study in unique_studies:\n",
    "        study_data = df_all_studies[df_all_studies['study'].astype(str).str.upper() == study].copy()\n",
    "        \n",
    "        if len(study_data) == 0:\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nProcessing Study {study} ({len(study_data)} participants)...\")\n",
    "        \n",
    "        for task in ['qIAT', 'IAT']:\n",
    "            task_data = study_data[study_data['task'].str.upper() == task.upper()].copy()\n",
    "            \n",
    "            if len(task_data) == 0:\n",
    "                continue\n",
    "            \n",
    "            print(f\"  Processing {task} ({len(task_data)} participants)...\")\n",
    "            \n",
    "            # Calculate Time1 baseline statistics (mean and SD) for this task\n",
    "            t1_scores = task_data['t1_dscore'].dropna()\n",
    "            \n",
    "            if len(t1_scores) == 0:\n",
    "                print(f\"    Warning: No valid T1 scores for {task} in Study {study}\")\n",
    "                continue\n",
    "            \n",
    "            t1_mean = t1_scores.mean()\n",
    "            t1_sd = t1_scores.std(ddof=1)  # Sample standard deviation\n",
    "            \n",
    "            print(f\"    T1 baseline: Mean = {t1_mean:.4f}, SD = {t1_sd:.4f}\")\n",
    "            \n",
    "            # Standardize both T1 and T2 scores using T1 baseline statistics\n",
    "            # Formula: z = (x - mean_t1) / sd_t1\n",
    "            task_indices = task_data.index\n",
    "            \n",
    "            # Standardize T1 scores\n",
    "            df_all_studies.loc[task_indices, 't1_com_d_norm'] = (task_data['t1_dscore'] - t1_mean) / t1_sd\n",
    "            \n",
    "            # Standardize T2 scores\n",
    "            df_all_studies.loc[task_indices, 't2_com_d_norm'] = (task_data['t2_dscore'] - t1_mean) / t1_sd\n",
    "            \n",
    "            # Report some statistics\n",
    "            t1_norm_computed = df_all_studies.loc[task_indices, 't1_com_d_norm'].notna().sum()\n",
    "            t2_norm_computed = df_all_studies.loc[task_indices, 't2_com_d_norm'].notna().sum()\n",
    "            print(f\"    Computed normalized scores: T1={t1_norm_computed}, T2={t2_norm_computed}\")\n",
    "\n",
    "print(f\"\\nFinal columns: {len(df_all_studies.columns)}\")\n",
    "\n",
    "# Save the updated file\n",
    "print(f\"\\nSaving to {combined_file}...\")\n",
    "df_all_studies.to_csv(combined_file, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a30c61b",
   "metadata": {},
   "source": [
    "## Comparison of Faking Effects Across Measures (Normalized Scores)\n",
    "\n",
    "Compare faking effects between self-report, IAT, and qIAT using normalized scores. The faking effect is computed as the difference between normalized T2 and T1 scores (T2_norm - T1_norm) for each measure. This enables direct comparison of faking effects across measures since all scores are standardized using Time1 baseline statistics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "703c636e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questionnaire T1 baseline: Mean = 27.9073, SD = 8.7368\n",
      "\n",
      "============================================================\n",
      "Comparison of Faking Effects Across Measures\n",
      "============================================================\n",
      "\n",
      "1. Self-Report vs IAT (Paired t-tests)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Faking Low group (n = 47):\n",
      "  Self-report: M = -1.50, SD = 1.03\n",
      "  IAT: M = 0.09, SD = 0.88\n",
      "  Paired t-test: t(46) = -9.40, p = 0.0000\n",
      "  Cohen's d = -1.37\n",
      "\n",
      "Faking High group (n = 46):\n",
      "  Self-report: M = 2.26, SD = 0.98\n",
      "  IAT: M = 0.39, SD = 0.78\n",
      "  Paired t-test: t(45) = 9.96, p = 0.0000\n",
      "  Cohen's d = 1.47\n",
      "\n",
      "\n",
      "2. Self-Report vs qIAT (Paired t-tests)\n",
      "------------------------------------------------------------\n",
      "\n",
      "Faking Low group (n = 38):\n",
      "  Self-report: M = -1.74, SD = 1.13\n",
      "  qIAT: M = -0.04, SD = 0.69\n",
      "  Paired t-test: t(37) = -9.29, p = 0.0000\n",
      "  Cohen's d = -1.51\n",
      "\n",
      "Faking High group (n = 39):\n",
      "  Self-report: M = 2.15, SD = 1.24\n",
      "  qIAT: M = 0.23, SD = 0.83\n",
      "  Paired t-test: t(38) = 9.79, p = 0.0000\n",
      "  Cohen's d = 1.57\n",
      "\n",
      "\n",
      "3. IAT vs qIAT (Independent t-tests)\n",
      "------------------------------------------------------------\n",
      "\n",
      "Faking Low group:\n",
      "  IAT: M = 0.09, SD = 0.88, n = 47\n",
      "  qIAT: M = -0.04, SD = 0.69, n = 38\n",
      "  Independent t-test: t(83) = 0.75, p = 0.4551\n",
      "  Cohen's d = 0.16\n",
      "\n",
      "Faking High group:\n",
      "  IAT: M = 0.39, SD = 0.78, n = 46\n",
      "  qIAT: M = 0.23, SD = 0.83, n = 39\n",
      "  Independent t-test: t(83) = 0.88, p = 0.3790\n",
      "  Cohen's d = 0.19\n",
      "\n",
      "============================================================\n",
      "SUMMARY (for article):\n",
      "============================================================\n",
      "\n",
      "Self-Report vs IAT:\n",
      "  Faking Low: t(46) = -9.40, p < .001, d = -1.37\n",
      "  Faking High: t(45) = 9.96, p < .001, d = 1.47\n",
      "\n",
      "Self-Report vs qIAT:\n",
      "  Faking Low: t(37) = -9.29, p < .001, d = -1.51\n",
      "  Faking High: t(38) = 9.79, p < .001, d = 1.57\n",
      "\n",
      "IAT vs qIAT:\n",
      "  Faking Low: t(83) = 0.75, p = 0.455, d = 0.16\n",
      "  Faking High: t(83) = 0.88, p = 0.379, d = 0.19\n"
     ]
    }
   ],
   "source": [
    "# First, compute normalized questionnaire scores using T1 baseline statistics\n",
    "# Then compute faking effects (T2_norm - T1_norm) and compare across measures\n",
    "\n",
    "# Reload data to get updated normalized D-scores\n",
    "df_updated = df.copy()\n",
    "\n",
    "# Compute normalized questionnaire scores\n",
    "# Calculate T1 baseline statistics for questionnaire (across all participants)\n",
    "t1_ques_all = pd.to_numeric(df_updated['t1_ques'], errors='coerce').dropna()\n",
    "t1_ques_mean = t1_ques_all.mean()\n",
    "t1_ques_sd = t1_ques_all.std(ddof=1)\n",
    "\n",
    "print(f\"Questionnaire T1 baseline: Mean = {t1_ques_mean:.4f}, SD = {t1_ques_sd:.4f}\")\n",
    "\n",
    "# Standardize questionnaire scores\n",
    "df_updated['t1_ques_norm'] = (pd.to_numeric(df_updated['t1_ques'], errors='coerce') - t1_ques_mean) / t1_ques_sd\n",
    "df_updated['t2_ques_norm'] = (pd.to_numeric(df_updated['t2_ques'], errors='coerce') - t1_ques_mean) / t1_ques_sd\n",
    "\n",
    "# Compute faking effects (T2_norm - T1_norm) for questionnaire\n",
    "df_updated['ques_faking_effect'] = df_updated['t2_ques_norm'] - df_updated['t1_ques_norm']\n",
    "\n",
    "# For IAT and qIAT, compute faking effects from normalized D-scores\n",
    "# Note: We need to merge IAT and qIAT data with questionnaire data\n",
    "# Since participants have either IAT or qIAT, we'll process them separately\n",
    "\n",
    "# Prepare IAT data\n",
    "iat_data = df_updated[df_updated['task'].str.upper() == 'IAT'].copy()\n",
    "iat_data['iat_faking_effect'] = (pd.to_numeric(iat_data['t2_com_d_norm'], errors='coerce') - \n",
    "                                  pd.to_numeric(iat_data['t1_com_d_norm'], errors='coerce'))\n",
    "\n",
    "# Prepare qIAT data\n",
    "qiat_data = df_updated[df_updated['task'].str.upper() == 'QIAT'].copy()\n",
    "qiat_data['qiat_faking_effect'] = (pd.to_numeric(qiat_data['t2_com_d_norm'], errors='coerce') - \n",
    "                                    pd.to_numeric(qiat_data['t1_com_d_norm'], errors='coerce'))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Comparison of Faking Effects Across Measures\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. Self-Report vs IAT (Paired t-tests)\n",
    "# ============================================================================\n",
    "print(\"\\n1. Self-Report vs IAT (Paired t-tests)\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Merge questionnaire with IAT data by participant ID\n",
    "iat_comparison = iat_data[['id', 'group', 'iat_faking_effect']].copy()\n",
    "ques_for_iat = df_updated[['id', 'ques_faking_effect']].copy()\n",
    "iat_comparison = iat_comparison.merge(ques_for_iat, on='id', how='inner')\n",
    "\n",
    "# Faking Low group (group == 1)\n",
    "iat_fl = iat_comparison[iat_comparison['group'] == 1].copy()\n",
    "iat_fl = iat_fl.dropna(subset=['ques_faking_effect', 'iat_faking_effect'])\n",
    "\n",
    "if len(iat_fl) > 0:\n",
    "    ques_fl_mean = iat_fl['ques_faking_effect'].mean()\n",
    "    ques_fl_sd = iat_fl['ques_faking_effect'].std(ddof=1)\n",
    "    iat_fl_mean = iat_fl['iat_faking_effect'].mean()\n",
    "    iat_fl_sd = iat_fl['iat_faking_effect'].std(ddof=1)\n",
    "    \n",
    "    # Paired t-test\n",
    "    t_stat, p_val = ttest_rel(iat_fl['ques_faking_effect'], iat_fl['iat_faking_effect'])\n",
    "    \n",
    "    # Cohen's d for paired samples\n",
    "    diff = iat_fl['ques_faking_effect'] - iat_fl['iat_faking_effect']\n",
    "    cohens_d = diff.mean() / diff.std(ddof=1)\n",
    "    \n",
    "    print(f\"\\nFaking Low group (n = {len(iat_fl)}):\")\n",
    "    print(f\"  Self-report: M = {ques_fl_mean:.2f}, SD = {ques_fl_sd:.2f}\")\n",
    "    print(f\"  IAT: M = {iat_fl_mean:.2f}, SD = {iat_fl_sd:.2f}\")\n",
    "    print(f\"  Paired t-test: t({len(iat_fl)-1}) = {t_stat:.2f}, p = {p_val:.4f}\")\n",
    "    print(f\"  Cohen's d = {cohens_d:.2f}\")\n",
    "\n",
    "# Faking High group (group == 2)\n",
    "iat_fh = iat_comparison[iat_comparison['group'] == 2].copy()\n",
    "iat_fh = iat_fh.dropna(subset=['ques_faking_effect', 'iat_faking_effect'])\n",
    "\n",
    "if len(iat_fh) > 0:\n",
    "    ques_fh_mean = iat_fh['ques_faking_effect'].mean()\n",
    "    ques_fh_sd = iat_fh['ques_faking_effect'].std(ddof=1)\n",
    "    iat_fh_mean = iat_fh['iat_faking_effect'].mean()\n",
    "    iat_fh_sd = iat_fh['iat_faking_effect'].std(ddof=1)\n",
    "    \n",
    "    # Paired t-test\n",
    "    t_stat, p_val = ttest_rel(iat_fh['ques_faking_effect'], iat_fh['iat_faking_effect'])\n",
    "    \n",
    "    # Cohen's d for paired samples\n",
    "    diff = iat_fh['ques_faking_effect'] - iat_fh['iat_faking_effect']\n",
    "    cohens_d = diff.mean() / diff.std(ddof=1)\n",
    "    \n",
    "    print(f\"\\nFaking High group (n = {len(iat_fh)}):\")\n",
    "    print(f\"  Self-report: M = {ques_fh_mean:.2f}, SD = {ques_fh_sd:.2f}\")\n",
    "    print(f\"  IAT: M = {iat_fh_mean:.2f}, SD = {iat_fh_sd:.2f}\")\n",
    "    print(f\"  Paired t-test: t({len(iat_fh)-1}) = {t_stat:.2f}, p = {p_val:.4f}\")\n",
    "    print(f\"  Cohen's d = {cohens_d:.2f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. Self-Report vs qIAT (Paired t-tests)\n",
    "# ============================================================================\n",
    "print(\"\\n\\n2. Self-Report vs qIAT (Paired t-tests)\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Merge questionnaire with qIAT data by participant ID\n",
    "qiat_comparison = qiat_data[['id', 'group', 'qiat_faking_effect']].copy()\n",
    "ques_for_qiat = df_updated[['id', 'ques_faking_effect']].copy()\n",
    "qiat_comparison = qiat_comparison.merge(ques_for_qiat, on='id', how='inner')\n",
    "\n",
    "# Faking Low group (group == 1)\n",
    "qiat_fl = qiat_comparison[qiat_comparison['group'] == 1].copy()\n",
    "qiat_fl = qiat_fl.dropna(subset=['ques_faking_effect', 'qiat_faking_effect'])\n",
    "\n",
    "if len(qiat_fl) > 0:\n",
    "    ques_fl_mean = qiat_fl['ques_faking_effect'].mean()\n",
    "    ques_fl_sd = qiat_fl['ques_faking_effect'].std(ddof=1)\n",
    "    qiat_fl_mean = qiat_fl['qiat_faking_effect'].mean()\n",
    "    qiat_fl_sd = qiat_fl['qiat_faking_effect'].std(ddof=1)\n",
    "    \n",
    "    # Paired t-test\n",
    "    t_stat, p_val = ttest_rel(qiat_fl['ques_faking_effect'], qiat_fl['qiat_faking_effect'])\n",
    "    \n",
    "    # Cohen's d for paired samples\n",
    "    diff = qiat_fl['ques_faking_effect'] - qiat_fl['qiat_faking_effect']\n",
    "    cohens_d = diff.mean() / diff.std(ddof=1)\n",
    "    \n",
    "    print(f\"\\nFaking Low group (n = {len(qiat_fl)}):\")\n",
    "    print(f\"  Self-report: M = {ques_fl_mean:.2f}, SD = {ques_fl_sd:.2f}\")\n",
    "    print(f\"  qIAT: M = {qiat_fl_mean:.2f}, SD = {qiat_fl_sd:.2f}\")\n",
    "    print(f\"  Paired t-test: t({len(qiat_fl)-1}) = {t_stat:.2f}, p = {p_val:.4f}\")\n",
    "    print(f\"  Cohen's d = {cohens_d:.2f}\")\n",
    "\n",
    "# Faking High group (group == 2)\n",
    "qiat_fh = qiat_comparison[qiat_comparison['group'] == 2].copy()\n",
    "qiat_fh = qiat_fh.dropna(subset=['ques_faking_effect', 'qiat_faking_effect'])\n",
    "\n",
    "if len(qiat_fh) > 0:\n",
    "    ques_fh_mean = qiat_fh['ques_faking_effect'].mean()\n",
    "    ques_fh_sd = qiat_fh['ques_faking_effect'].std(ddof=1)\n",
    "    qiat_fh_mean = qiat_fh['qiat_faking_effect'].mean()\n",
    "    qiat_fh_sd = qiat_fh['qiat_faking_effect'].std(ddof=1)\n",
    "    \n",
    "    # Paired t-test\n",
    "    t_stat, p_val = ttest_rel(qiat_fh['ques_faking_effect'], qiat_fh['qiat_faking_effect'])\n",
    "    \n",
    "    # Cohen's d for paired samples\n",
    "    diff = qiat_fh['ques_faking_effect'] - qiat_fh['qiat_faking_effect']\n",
    "    cohens_d = diff.mean() / diff.std(ddof=1)\n",
    "    \n",
    "    print(f\"\\nFaking High group (n = {len(qiat_fh)}):\")\n",
    "    print(f\"  Self-report: M = {ques_fh_mean:.2f}, SD = {ques_fh_sd:.2f}\")\n",
    "    print(f\"  qIAT: M = {qiat_fh_mean:.2f}, SD = {qiat_fh_sd:.2f}\")\n",
    "    print(f\"  Paired t-test: t({len(qiat_fh)-1}) = {t_stat:.2f}, p = {p_val:.4f}\")\n",
    "    print(f\"  Cohen's d = {cohens_d:.2f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. IAT vs qIAT (Independent t-tests)\n",
    "# ============================================================================\n",
    "print(\"\\n\\n3. IAT vs qIAT (Independent t-tests)\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Faking Low group (group == 1)\n",
    "iat_fl_ind = iat_data[iat_data['group'] == 1].copy()\n",
    "iat_fl_ind = iat_fl_ind.dropna(subset=['iat_faking_effect'])\n",
    "qiat_fl_ind = qiat_data[qiat_data['group'] == 1].copy()\n",
    "qiat_fl_ind = qiat_fl_ind.dropna(subset=['qiat_faking_effect'])\n",
    "\n",
    "if len(iat_fl_ind) > 0 and len(qiat_fl_ind) > 0:\n",
    "    iat_fl_mean = iat_fl_ind['iat_faking_effect'].mean()\n",
    "    iat_fl_sd = iat_fl_ind['iat_faking_effect'].std(ddof=1)\n",
    "    qiat_fl_mean = qiat_fl_ind['qiat_faking_effect'].mean()\n",
    "    qiat_fl_sd = qiat_fl_ind['qiat_faking_effect'].std(ddof=1)\n",
    "    \n",
    "    # Independent t-test\n",
    "    t_stat, p_val = ttest_ind(iat_fl_ind['iat_faking_effect'], qiat_fl_ind['qiat_faking_effect'])\n",
    "    \n",
    "    # Cohen's d for independent samples\n",
    "    pooled_sd = np.sqrt(((len(iat_fl_ind) - 1) * iat_fl_sd**2 + (len(qiat_fl_ind) - 1) * qiat_fl_sd**2) / \n",
    "                        (len(iat_fl_ind) + len(qiat_fl_ind) - 2))\n",
    "    cohens_d = (iat_fl_mean - qiat_fl_mean) / pooled_sd\n",
    "    \n",
    "    print(f\"\\nFaking Low group:\")\n",
    "    print(f\"  IAT: M = {iat_fl_mean:.2f}, SD = {iat_fl_sd:.2f}, n = {len(iat_fl_ind)}\")\n",
    "    print(f\"  qIAT: M = {qiat_fl_mean:.2f}, SD = {qiat_fl_sd:.2f}, n = {len(qiat_fl_ind)}\")\n",
    "    print(f\"  Independent t-test: t({len(iat_fl_ind) + len(qiat_fl_ind) - 2}) = {t_stat:.2f}, p = {p_val:.4f}\")\n",
    "    print(f\"  Cohen's d = {cohens_d:.2f}\")\n",
    "\n",
    "# Faking High group (group == 2)\n",
    "iat_fh_ind = iat_data[iat_data['group'] == 2].copy()\n",
    "iat_fh_ind = iat_fh_ind.dropna(subset=['iat_faking_effect'])\n",
    "qiat_fh_ind = qiat_data[qiat_data['group'] == 2].copy()\n",
    "qiat_fh_ind = qiat_fh_ind.dropna(subset=['qiat_faking_effect'])\n",
    "\n",
    "if len(iat_fh_ind) > 0 and len(qiat_fh_ind) > 0:\n",
    "    iat_fh_mean = iat_fh_ind['iat_faking_effect'].mean()\n",
    "    iat_fh_sd = iat_fh_ind['iat_faking_effect'].std(ddof=1)\n",
    "    qiat_fh_mean = qiat_fh_ind['qiat_faking_effect'].mean()\n",
    "    qiat_fh_sd = qiat_fh_ind['qiat_faking_effect'].std(ddof=1)\n",
    "    \n",
    "    # Independent t-test\n",
    "    t_stat, p_val = ttest_ind(iat_fh_ind['iat_faking_effect'], qiat_fh_ind['qiat_faking_effect'])\n",
    "    \n",
    "    # Cohen's d for independent samples\n",
    "    pooled_sd = np.sqrt(((len(iat_fh_ind) - 1) * iat_fh_sd**2 + (len(qiat_fh_ind) - 1) * qiat_fh_sd**2) / \n",
    "                        (len(iat_fh_ind) + len(qiat_fh_ind) - 2))\n",
    "    cohens_d = (iat_fh_mean - qiat_fh_mean) / pooled_sd\n",
    "    \n",
    "    print(f\"\\nFaking High group:\")\n",
    "    print(f\"  IAT: M = {iat_fh_mean:.2f}, SD = {iat_fh_sd:.2f}, n = {len(iat_fh_ind)}\")\n",
    "    print(f\"  qIAT: M = {qiat_fh_mean:.2f}, SD = {qiat_fh_sd:.2f}, n = {len(qiat_fh_ind)}\")\n",
    "    print(f\"  Independent t-test: t({len(iat_fh_ind) + len(qiat_fh_ind) - 2}) = {t_stat:.2f}, p = {p_val:.4f}\")\n",
    "    print(f\"  Cohen's d = {cohens_d:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY (for article):\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nSelf-Report vs IAT:\")\n",
    "if len(iat_fl) > 0:\n",
    "    print(f\"  Faking Low: t({len(iat_fl)-1}) = {ttest_rel(iat_fl['ques_faking_effect'], iat_fl['iat_faking_effect'])[0]:.2f}, p < .001, d = {cohens_d_paired(iat_fl['ques_faking_effect'], iat_fl['iat_faking_effect']):.2f}\")\n",
    "if len(iat_fh) > 0:\n",
    "    print(f\"  Faking High: t({len(iat_fh)-1}) = {ttest_rel(iat_fh['ques_faking_effect'], iat_fh['iat_faking_effect'])[0]:.2f}, p < .001, d = {cohens_d_paired(iat_fh['ques_faking_effect'], iat_fh['iat_faking_effect']):.2f}\")\n",
    "\n",
    "print(\"\\nSelf-Report vs qIAT:\")\n",
    "if len(qiat_fl) > 0:\n",
    "    print(f\"  Faking Low: t({len(qiat_fl)-1}) = {ttest_rel(qiat_fl['ques_faking_effect'], qiat_fl['qiat_faking_effect'])[0]:.2f}, p < .001, d = {cohens_d_paired(qiat_fl['ques_faking_effect'], qiat_fl['qiat_faking_effect']):.2f}\")\n",
    "if len(qiat_fh) > 0:\n",
    "    print(f\"  Faking High: t({len(qiat_fh)-1}) = {ttest_rel(qiat_fh['ques_faking_effect'], qiat_fh['qiat_faking_effect'])[0]:.2f}, p < .001, d = {cohens_d_paired(qiat_fh['ques_faking_effect'], qiat_fh['qiat_faking_effect']):.2f}\")\n",
    "\n",
    "print(\"\\nIAT vs qIAT:\")\n",
    "if len(iat_fl_ind) > 0 and len(qiat_fl_ind) > 0:\n",
    "    pooled_sd_fl = np.sqrt(((len(iat_fl_ind) - 1) * iat_fl_ind['iat_faking_effect'].std(ddof=1)**2 + (len(qiat_fl_ind) - 1) * qiat_fl_ind['qiat_faking_effect'].std(ddof=1)**2) / (len(iat_fl_ind) + len(qiat_fl_ind) - 2))\n",
    "    d_fl = (iat_fl_ind['iat_faking_effect'].mean() - qiat_fl_ind['qiat_faking_effect'].mean()) / pooled_sd_fl\n",
    "    print(f\"  Faking Low: t({len(iat_fl_ind) + len(qiat_fl_ind) - 2}) = {ttest_ind(iat_fl_ind['iat_faking_effect'], qiat_fl_ind['qiat_faking_effect'])[0]:.2f}, p = {ttest_ind(iat_fl_ind['iat_faking_effect'], qiat_fl_ind['qiat_faking_effect'])[1]:.3f}, d = {d_fl:.2f}\")\n",
    "if len(iat_fh_ind) > 0 and len(qiat_fh_ind) > 0:\n",
    "    pooled_sd_fh = np.sqrt(((len(iat_fh_ind) - 1) * iat_fh_ind['iat_faking_effect'].std(ddof=1)**2 + (len(qiat_fh_ind) - 1) * qiat_fh_ind['qiat_faking_effect'].std(ddof=1)**2) / (len(iat_fh_ind) + len(qiat_fh_ind) - 2))\n",
    "    d_fh = (iat_fh_ind['iat_faking_effect'].mean() - qiat_fh_ind['qiat_faking_effect'].mean()) / pooled_sd_fh\n",
    "    print(f\"  Faking High: t({len(iat_fh_ind) + len(qiat_fh_ind) - 2}) = {ttest_ind(iat_fh_ind['iat_faking_effect'], qiat_fh_ind['qiat_faking_effect'])[0]:.2f}, p = {ttest_ind(iat_fh_ind['iat_faking_effect'], qiat_fh_ind['qiat_faking_effect'])[1]:.3f}, d = {d_fh:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
